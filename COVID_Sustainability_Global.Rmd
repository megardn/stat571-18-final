---
title: "COVID_Sustainability_Global"
author:
- Diego G. Davila
- Margaret Gardner
- Joelle Bagautdinova
date: 'Due before midnight, May 1st'
output:
  html_document:
    code_folding: show
    highlight: haddock
    theme: lumen
    toc: yes
    toc_depth: 4
    toc_float: yes
    number_sections: yes
  pdf_document:
    toc_depth: '4'
    number_sections: yes
urlcolor: blue
---

```{r Setup, include=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width=8, fig.height=4)
options(scipen = 0, digits = 3)  # controls base R output

# Package setup
if(!require("pacman")) install.packages("pacman")

pacman::p_load(tidyverse, dplyr, ggplot2, ggthemes, data.table, lubridate,
               GGally, RColorBrewer, ggsci, plotROC, usmap,
               plotly, ggpubr, vistime, skimr, glmnet, dgCMatrix, leaps, car, corrplot)
```



Load outputs from EDA Rmds
```{r load.dfs}
load("data/WorldSustainabilityData_Processed.Rda")
#skim(wsd)
max(wsd$Year)
wsd <- wsd %>% dplyr::select(1:3,"Continent", "WorldRegion", 4:21,23:31) %>% # rearrange a little to group continent & World Region with the other geographic data
  mutate(Country.Name=as.factor(Country.Name)) %>%
  rename(country=Country.Name)

load("data/covid_cases_vaccines_clean.Rda")
#skim(covid)
```

```{r}
#some discrepancy in how countries are named, gonna fix that
wsd.country <- levels(wsd$country)
covid.country <- levels(covid$country)
#setdiff(wsd.country, covid.country) #only in wsd, not covid
#setdiff(covid.country, wsd.country) #only in covid, not wsd

covid$country <- recode_factor(covid$country, "UK"="United Kingdom", "Gambia"="Gambia, The", "Laos"="Lao PDR", "Egypt"="Egypt, Arab Rep.", "South Korea"="Korea, Rep.", "USA"="United States", "Russia"="Russian Federation", "Venezuela"="Venezuela, RB", "Democratic Republic Of The Congo"="DRC")
wsd$country <- recode_factor(wsd$country,  "Iran, Islamic Rep."="Iran", "Bosnia and Herzegovina"="Bosnia And Herzegovina", "Cote d'Ivoire"="Cote D Ivoire", "Guinea-Bissau"="Guinea Bissau", "Kyrgyz Republic"="Kyrgyzstan", "North Macedonia"="Macedonia", "Eswatini"="Swaziland", "Vietnam"="Viet Nam", "Timor-Leste"="Timor Leste", "Slovak Republic"="Slovakia", "Congo, Rep."="Congo", "Congo, Dem. Rep."="DRC")
```

Before attempting to merge, I'll parse down the dataframes so we can hopefully we can mix and match as needed to get what we need for our final analyses.

## WSD Summary Vales

To start, I'll make a wide df the WSD data into the COVID dataset containing: the latest (2018) data; average of each parameter over time (2001 to 2018); the average of the last 3 years of data (2016-2018 for most countries); change in each parameter over time (2001-2018). Also made one for latest date for each country since 32 are missing 2018 data, not sure which we want to use!

``` {r}
# 2018 values
wsd.2018 <- wsd %>% subset(Year==max(wsd$Year)) %>% dplyr::select(-Year) #filter to last year of WSD data
colnames(wsd.2018)[5:30] <- paste(colnames(wsd.2018)[5:30], "2018", sep = "_") #add suffix
head(wsd.2018)
n_unique(wsd.2018$country) 
```

```{r}
# latest data for each country
wsd.latest <- wsd %>% arrange(Year) %>%
  group_by(country) %>%
  slice_tail(n=1) %>%
  ungroup()
colnames(wsd.latest)[2:31] <- paste(colnames(wsd.latest)[2:31], "latest", sep = "_") #add suffix
table(as.factor(wsd.latest$Year_latest)) # see yr frequencies
```

Values averaged across 2016-2018; only includes countries with da. 

```{r}
# 3-yr averages for numeric
wsd.3yr.num <- wsd %>% arrange(Year) %>%
  filter(Year >= 2016) %>% #get 2016-2018 data
  group_by(country) %>%
  filter(n() == 3) %>%
  summarise_if(is.numeric, mean) %>% #means
  ungroup()
colnames(wsd.3yr.num)[2:26] <- paste(colnames(wsd.3yr.num)[2:26], "3yr", sep = "_") #add suffix


# 3-yr mode for factors
# create a function to find the mode. Taken from: https://stackoverflow.com/questions/2547402/how-to-find-the-statistical-mode
Mode <- function(x) {
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}

wsd.3yr.fac <- wsd %>% arrange(Year) %>%
  group_by(country) %>%
  filter(Year >= 2016) %>% #get 2016-2018 data
  filter(n() == 3) %>%
  mutate(WorldBankIncomeClass = Mode(WorldBankIncomeClass),
         RegimeType = Mode(RegimeType),
         nYears=n()) %>% #count how many years are being averaged for each country) %>%
  ungroup() %>%
  dplyr::select(country, WorldRegion, nYears, WorldBankIncomeClass, RegimeType) %>%
  distinct()
#n_unique(wsd.3yr.fac$country) #make sure no countries missing
colnames(wsd.3yr.fac)[3:5] <- paste(colnames(wsd.3yr.fac)[3:5], "3yr", sep = "_") #add suffix

wsd.3yr <- merge(wsd.3yr.fac, wsd.3yr.num, by="country") %>% #rejoin num and fac
    dplyr::select(-Year_3yr, -nYears_3yr)
nrow(wsd.3yr)
```
Averaging across all years for every country with available data. 

```{r}
# averages for numeric
wsd.avg.num <- wsd %>% arrange(Year) %>%
  group_by(country) %>%
  summarise_if(is.numeric, mean) %>%
  ungroup() %>%
  dplyr::select(-Year)
colnames(wsd.avg.num)[2:25] <- paste(colnames(wsd.avg.num)[2:25], "avg", sep = "_") #add suffix

# mode for factors
wsd.avg.fac <- wsd %>% arrange(Year) %>%
  group_by(country) %>%
  mutate(WorldBankIncomeClass = Mode(WorldBankIncomeClass),
         RegimeType = Mode(RegimeType),
         nYears=n()) %>% #count how many years are being averaged for each country
  ungroup() %>%
  dplyr::select(country, WorldRegion, WorldBankIncomeClass, RegimeType, nYears) %>%
  distinct()
colnames(wsd.avg.fac)[3:5] <- paste(colnames(wsd.avg.fac)[3:5], "avg", sep = "_") #add suffix

# #make sure modes returned as expected 
# angola <- wsd$WorldBankIncomeClass[wsd$country=="Angola"]
# Mode(angola)
# wsd.avg.fac$WorldBankIncomeClass[wsd.avg.fac$country=="Angola"]
# #looks good!

wsd.avg <- merge(wsd.avg.fac, wsd.avg.num, by="country") #rejoin num and fac
table(as.factor(wsd.avg$nYears_avg)) # see yr frequencies
```

As you can see from the frequency table above, only 60 countries have data for all 18 years. We could restrict this to only including the averages of countries with at least 15 yrs of data by uncommenting the chunk below (filters df down to 99 countries):

```{r}
# #uncomment to remove any countries that have less than 15 years worth of data
# wsd.avg <- wsd.avg %>%
#   subset(nYears_avg >= 15)
# nrow(wsd.avg)
```


Change in each factor over 10 years (2008-2018)

```{r}
# change in each parameter over 10 years

# define function to get relative change
rel.diff <- function(x) {(x - lag(x))/lag(x)} # define function to get relative change


wsd.change.num <- wsd %>% 
  group_by(country) %>%
  arrange(Year) %>%
  select_if(is.numeric)%>% # just dealing with numeric for now
  subset(Year==2018 | Year == 2008) %>%
  filter(n() >= 2) %>% #make sure country has data for both years
  mutate(across(where(is.numeric), ~rel.diff(.), .names="{.col}_rel.diff")) %>% # change normalized by baseline values
  mutate(across(c(!ends_with("diff")), ~diff(.x), .names="{.col}_diff"))  %>%  #absolute change 
  mutate_if(is.numeric, function(x) ifelse(is.infinite(x), 0, x)) %>% # change Inf values to 0
  dplyr::select(-Year, -Year_diff, -Year_rel.diff) %>%
  drop_na() %>%
  ungroup()

#now factors
wsd.change.fac <- wsd %>%
  dplyr::select(country, Year, WorldBankIncomeClass, RegimeType)
# order factors
wsd.change.fac$WorldBankIncomeClass <- factor(wsd.change.fac$WorldBankIncomeClass, levels=c("Low income", "Lower-middle income", "Upper-middle income", "High income"), ordered=TRUE)
wsd.change.fac$RegimeType <- factor(wsd.change.fac$RegimeType, levels=c("Closed Autocracy", "Electoral Autocracy", "Electoral Democracy", "Liberal Democracy"), ordered=TRUE)

wsd.change.fac <- wsd.change.fac %>% 
  group_by(country) %>%
  arrange(Year) %>%
  subset(Year==2018 | Year == 2008) %>%
  filter(n() >= 2) %>% #make sure country has data for both years
  mutate(WorldBankIncomeClass_diff = unclass(WorldBankIncomeClass) - unclass(lag(WorldBankIncomeClass)),
         WorldBankIncomeClass_rel.diff = rel.diff(unclass(WorldBankIncomeClass)),
         RegimeType_diff = unclass(RegimeType) - unclass(lag(RegimeType)),
         RegimeType_rel.diff = rel.diff(unclass(RegimeType))) %>% 
  mutate_if(is.numeric, function(x) ifelse(is.infinite(x), 0, x)) %>% # change Inf values to 0
  dplyr::select(-Year) %>%
  drop_na() %>%
  ungroup()

#now join and drop unnecessary columns used for calculations
wsd.change <- merge(wsd.change.num, wsd.change.fac, by ="country") %>%
  dplyr::select(country, ends_with("diff"))
```

Merge into one wide wsd dataframe:

```{r}
# one predictor dataframe to rule them all
wsd.df_list <- list(wsd.2018, wsd.3yr, wsd.avg, wsd.change) #dfs to merge
wsd.sum.wide <- wsd.df_list %>% reduce(full_join, by='country') %>%
  dplyr::select(-c(WorldRegion.x, WorldRegion.y))#join
#skim(wsd.sum.wide)

#here's a long version if you want it
wsd.sum.long <- wsd.sum.wide %>%
  dplyr::select(-WorldBankIncomeClass_rel.diff, -RegimeType_rel.diff) %>%
  mutate(WorldBankIncomeClass_diff=as.factor(WorldBankIncomeClass_diff),
         RegimeType_diff=as.factor(RegimeType_diff)) %>%
  pivot_longer(c(contains("_")), 
    names_to = c(".value", "type"), 
    names_sep = "_", 
    values_drop_na = TRUE
  )

head(wsd.sum.wide)
```

## COVID Summary Values

I'll attempt to make the dataframe smaller and more mergable by filtering down into summary variables, similar to WSD. Some possibilities of things we could compare between countries:
- Date vaccinations start
- mean daily vaccinations during first 6 months of availability
- peak daily new deaths & date of that peak (actual date or number days from start)
- days to binned_people_vaccinated_per_hundred >= X
- binned_people_vaccinated_per_hundred as of March 2022
- percent vaccines as of March 2022

```{r}
skim(covid)
#last day with data for each country
last.timepoint1 <- covid %>% group_by(country) %>%
  arrange(country,date) %>%
  slice_tail(n=1) %>%
  ungroup()
last.timepoint_filt <- last.timepoint1[, which(colMeans(!is.na(last.timepoint1)) > 0.5)] #remove cols with NA > 50%
#skim(last.timepoint_filt)

# get missing column names
missing_vac_cols <-  names(which(colMeans(is.na(last.timepoint1)) > 0.5))

# add last reported (within 2022) vaccination info
vac_info_2022 <- covid %>%
  drop_na(people_vaccinated_per_hundred) %>%   # filter to keep only non-NA in people_vaccinated_per_hundred
  group_by(country) %>% 
  filter(date == max(date)) %>%   # keep last reported date for every country
  filter(year == 2022) %>% # keep only countries who last reported in 2022 (as we are interested in a recent vaccination status)
  # filter(month_year == "2022-03-01") # could also filter for march 2022 specifically
  dplyr::select(country, date, missing_vac_cols) %>% # keep only variables not yet included in last.timepoint
  rename(date.vac = date)# rename the date to correspond to last reported vaccine info

# merge the last reported vaccination info with last reported cases/deaths info
last.timepoint <- merge(last.timepoint_filt, vac_info_2022, by = "country")
colnames(last.timepoint)[2:43] <- paste(colnames(last.timepoint)[2:43], "last", sep = "_") #add suffix
#names(last.timepoint)

# #find date of vaccination events
# table(covid$vaccination_events)
# table(covid$vaccine_dose_events)
# table(covid$binned_people_vaccinated_per_hundred)
```

Vaccine outcomes:

```{r}
# creating dfs of vaccination events - much of this is from covid_EDA.Rmd and can be removed/cleaned up once Rmds are merged!
vac_start_df <- covid %>%
  dplyr::select(country, date, vaccination_events) %>%
  filter(vaccination_events == "vac_start") %>%
  mutate(days.to.start_rel = as.numeric(difftime(date, min(date), units = "days")),
         days.to.start_abs = as.numeric(difftime(date, "2020-12-01", units = "days"))) %>%
  rename(date.first.vac = date)

# adding date of first vaccination for each country to the covid df so it can be used as a baseline later -
vac_start_date <- vac_start_df %>% dplyr::select(country, date.first.vac)
covid <- full_join(covid, vac_start_date, by ="country")

#days until country admins 1 dose per person
vac_1_dose <- covid %>%
  dplyr::select(country, date, vaccine_dose_events, date.first.vac) %>% #
  filter(vaccine_dose_events == "1_dose")%>%
  mutate(days.to.1d_rel = as.numeric(difftime(date, min(date), units = "days")),
         days.to.1d_abs = as.numeric(difftime(date, "2020-12-01", units = "days")), # setting Dec 1, 2020 as vaccine baseline to compare across vaccine events
         days.to.1d_fromstart = as.numeric(difftime(date, date.first.vac, units = "days"))
         ) 
vac_1_dose[which.min(vac_1_dose$date),] #first country to get one dose per person

#days until country admins 2 doses per person
vac_2_doses <- covid %>%
  dplyr::select(country, date, vaccine_dose_events, date.first.vac) %>%
  filter(vaccine_dose_events == "2_doses")%>%
  mutate(days.to.2d_rel = as.numeric(difftime(date, min(date), units = "days")),
         days.to.2d_abs = as.numeric(difftime(date, "2020-12-01", units = "days")),
         days.to.2d_fromstart = as.numeric(difftime(date, date.first.vac, units = "days"))
         )
vac_2_doses[which.min(vac_2_doses$date),] #first country to hit 2 doses

#days until country reports vaccinating 10% of pop
vac_10_df <- covid %>%
  dplyr::select(country, date, vaccination_events, date.first.vac) %>%
  filter(vaccination_events == "vac_10pct") %>%
  mutate(days.to.10pct_rel = as.numeric(difftime(date, min(date), units = "days")),
         days.to.10pct_abs = as.numeric(difftime(date, "2020-12-01", units = "days")),
         days.to.10pct_fromstart = as.numeric(difftime(date, date.first.vac, units = "days"))
         )
nrow(vac_10_df)

#days until country reports vaccinating 20% of pop
vac_20_df <- covid %>%
  dplyr::select(country, date, vaccination_events, date.first.vac) %>%
  filter(vaccination_events == "vac_20pct") %>%
  mutate(days.to.20pct_rel = as.numeric(difftime(date, min(date), units = "days")),
         days.to.20pct_abs = as.numeric(difftime(date, "2020-12-01", units = "days")),
         days.to.20pct_fromstart = as.numeric(difftime(date, date.first.vac, units = "days"))
         )
nrow(vac_20_df)

#days until country reports vaccinating 30% of pop
vac_30_df <- covid %>%
  dplyr::select(country, date, vaccination_events, date.first.vac) %>%
  filter(vaccination_events == "vac_30pct") %>%
  mutate(days.to.30pct_rel = as.numeric(difftime(date, min(date), units = "days")),
         days.to.30pct_abs = as.numeric(difftime(date, "2020-12-01", units = "days")),
         days.to.30pct_fromstart = as.numeric(difftime(date, date.first.vac, units = "days"))
         )
nrow(vac_30_df)
n_unique(vac_30_df$country)

#days until country reports vaccinating 50% of pop
vac_50_df <- covid %>%
  dplyr::select(country, date, vaccination_events, date.first.vac) %>%
  filter(vaccination_events == "vac_50pct") %>%
  mutate(days.to.50pct_rel = as.numeric(difftime(date, min(date), units = "days")),
         days.to.50pct_abs = as.numeric(difftime(date, "2020-12-01", units = "days")),
         days.to.50pct_fromstart = as.numeric(difftime(date, date.first.vac, units = "days"))
         )

# #percent of ppl w/ 1 dose of vaccine as of mar 17 2022 - redundant to last.timepoint
# binned_vac <- covid %>%
#   dplyr::select(country, date, binned_people_vaccinated_per_hundred) %>%
#   drop_na(binned_people_vaccinated_per_hundred) %>%
#   group_by(country) %>%
#   arrange(date) %>%
#   slice_tail(n=1) %>%
#   ungroup() %>%
#   rename (bin.ppl.vac.per100_17mar22=binned_people_vaccinated_per_hundred)
#summary(binned_vac$date)
#n_unique(binned_vac$country) #no countries lost


# mean daily rate of vaccinations per million ppl in first 6 months of each country's administration
vac_rate <- covid %>%
  dplyr::select(country, date, daily_vaccinations_per_million, date.first.vac) %>% 
  filter(daily_vaccinations_per_million > 0,
         date <= (date.first.vac %m+% months(6))) %>% # daily vaccinations in 1st 6 mo - make relative to their own vaccine start date
  drop_na(daily_vaccinations_per_million) %>% 
  group_by(country) %>%
  summarise(daily.vac.per.mil_6moavg = mean(daily_vaccinations_per_million))


# max rate of vaccinations administration
vac_maxrate <- covid %>%
  dplyr::select(country, date, daily_vaccinations_per_million, daily_vaccinations_raw, daily_vaccinations) %>% 
  group_by(country) %>%
  mutate_if(is.numeric, ~replace(., is.na(.), -1)) %>% #temporarily replace na's with -1
  summarise_if(is.numeric, max) %>%
  na_if(-1) %>% #set -1's back to na
  rename(daily.vac.raw_max = daily_vaccinations_raw,
         daily.vac_max = daily_vaccinations,
         daily.vac.per.mil_max = daily_vaccinations_per_million)
  
```

``` {r}
# cases & deaths up until vaccines start
covid_prevac <- covid %>%
  group_by(country) %>%
  drop_na(date.first.vac) %>%
  subset(date < date.first.vac) %>%
  arrange(country,date) %>%
  slice_tail(n=1) %>%
  ungroup() %>%
  dplyr::select("country"| contains("case") | contains("death"))
colnames(covid_prevac)[2:19] <- paste(colnames(covid_prevac)[2:19], "prevac", sep = "_") #add suffix


# # cases & deaths at 1 yr after date.first.vac
# covid_1yr <- covid %>%
#   group_by(country) %>%
#   drop_na(date.first.vac) %>%
#   subset(date == (date.first.vac %m+% months(12))) %>%
#   dplyr::select("country"| contains("case") | contains("death"))
# colnames(covid_1yr)[2:19] <- paste(colnames(covid_1yr)[2:19], "1yr", sep = "_") #add suffix
# #covid_1yr <- covid_1yr[, which(colMeans(!is.na(covid_1yr)) > 0.5)] #remove cols with NA > 50%
# #sum(is.na(covid_1yr$cumulative_total_cases))

#peak rates up until vaccinations start
covid_maxrate <- covid %>%
  drop_na(date.first.vac) %>%
  subset(date < date.first.vac) %>% 
  dplyr::select("country" | contains(c("new", "daily"))) %>%
  dplyr::select(-contains("vac")) %>% #drop vac cols
  group_by(country) %>%
  mutate_if(is.numeric, ~replace(., is.na(.), -1)) %>% #temporarily replace na's with -1
  summarise_if(is.numeric, max) %>%
  na_if(-1) %>% #set -1's back to na
  rename(daily.new.cases.per.100 = daily_new_cases_per_hundred,
         daily.new.deaths.per.100 = daily_new_deaths_per_hundred,
         monthly.new.cases.per.100 = monthly_new_cases_per_hundred,
         monthly.new.deaths.per.100 = monthly_new_deaths_per_hundred,
         weekly.new.cases.per.100 = weekly_new_cases_per_hundred,
         weekly.new.deaths.per.100 = weekly_new_deaths_per_hundred) #using . and _ separators to match WSD in case we want to pivot long later
colnames(covid_maxrate)[2:13] <- paste(colnames(covid_maxrate)[2:13], "max", sep = "_") #add suffix

#n_unique(covid_maxrate$country) #one country lost filtering to pre "2021-02-20"

```


Now let's merge all these vaccination dfs into one dataframe we can try to predict!
First we'll dump all the vaccine-related stuff into one big dataframe here:

``` {r}
#big df for vaccine info
last.vac <- last.timepoint %>% dplyr::select("country"| contains("vacc"))#get just vaccine stuff from  last.timepoint
vac.df_list <- list(vac_1_dose, vac_2_doses, vac_start_df, vac_10_df, vac_20_df, vac_30_df, vac_50_df, vac_rate, vac_maxrate, last.vac)
#drop date and event info since this is contained in col names
for (i in 1:length(vac.df_list)) {
  vac.df_list[[i]] <- vac.df_list[[i]] %>% dplyr::select(-c(contains("date") | contains("events")))
}

vac.dump <- vac.df_list %>% reduce(full_join, by='country') %>% as.data.frame() #join
#skim(vac.dump)
```

Now we'll pair it down to keep only key variables that aren't redundant/highly correlated to run PCA on. NOTE: feel free to update/make changes to what's been kept!

```{r}
vac.sum <- vac.dump %>% dplyr::select(
  country, 
  days.to.start_rel, #days until a country reports starting to administer vaccines, with the 1st country to start vaccinating as a baseline
  days.to.10pct_fromstart, #days between starting vaccinations and administering to 10% of pop
  daily.vac.per.mil_6moavg, #mean daily vaccinations in the first 6 months of administering
  people_vaccinated_per_hundred_last, #number of ppl vaccinated (at least once) at last report (sometime in 2022)
  daily.vac.per.mil_max, #max number of vaccinations reported in a single day
  total_vaccinations_per_hundred_last,# number of vaccine doses at last report (sometime in 2022)
  people_fully_vaccinated_per_hundred_last # number of people that received the full vaccine (typically 2 doses)
)

write.csv(vac.sum, "data/vaccination_summary.csv", row.names = FALSE)
skim(vac.dump)
```


Merging case & death dfs

```{r}
#df for cases and deaths
last.case <- last.timepoint %>% dplyr::select("country", "date_last" | contains("case") | contains("death")) #get just cases & deaths stuff from  last.timepoint
case.df_list <- list(covid_prevac, covid_maxrate, last.case) #covid_1yr
case.sum.wide <- case.df_list %>% reduce(full_join, by='country') %>% as.data.frame() #join
#skim(case.sum.wide)
```

Merge all COVID summary data

```{r}
covid.df_list <- list(case.sum.wide, vac.dump)
covid.all <- covid.df_list %>% reduce(full_join, by='country') %>% as.data.frame() #join
n_unique(covid.all$country)
#skim(covid.all)
```


## Merge! Merge! Merge!

``` {r}
#merging ALL WSD predictors with cases, deaths & maxrate data for big, data-driven models!
case.wsd.wide <- inner_join(case.sum.wide, wsd.sum.wide, by="country")
#skim(case.wsd.wide)

#merging WSD predictors with vaccination summary data
vac.wsd.wide <- inner_join(vac.dump, wsd.sum.wide, by="country")
#skim(vac.wsd.wide)

#merging ALL WSD predictors with cases, deaths & maxrate data for big, data-driven models!
case.wsd.wide <- inner_join(case.sum.wide, wsd.sum.wide, by="country")
#skim(case.wsd.wide)

#merging WSD predictors with latest COVID data (~March 17, 2022)
mar22.wsd.wide <- inner_join(last.timepoint, wsd.sum.wide, by="country")
#skim(mar22.wsd.wide)

#merging everything!
wsd.covid.all <- inner_join(covid.all, wsd.sum.wide, by="country")
#skim(wsd.covid.all)
```

Any of the above can be recombined/merged further or pivoted long.

predict:
days to vaccine access
days from vaccine access to 50% ppl
% ppl vaccinated at march 2022 (absolute counts and binned)
doses administered at march 2022 (absolute counts and binned)

## Computing Vaccination Status Index

**add Joelle's PCA here**

## Modeling


In this section, we are using socioeconomic factors from `case.wsd.wide` to predict Vaccination Success Index (VSI). Note that VSI corresponds the inverse signed PC1 scores (from `data/VaccinationSucessIndexData-New.csv`) using and thus indicates how well a country did at getting people vaccinated, i.e. a positive score indicates a good vaccination performance, while a negative score indicates a poor vaccination performance. 

```{r}
#read in
new.data.with.VSI <- read.csv("data/VaccinationSucessIndexData-New.csv")


#adding in PC1
yval <- new.data.with.VSI %>% dplyr::select("country", "VSI")
df <- inner_join(case.wsd.wide, yval,  by=("country"))
#names(df)
n_unique(df$country)
```


Subdividing our dataset of 108 countries into `data.train`(60%), `data.test` (20%), and `data.val` (20%) 
```{r}

# numerical summary
skim(new.data.with.VSI)

# plot sorted VSI scores
new.data.with.VSI %>%
  ggplot(aes(x = reorder(country, VSI), 
               y = VSI, fill = country)) +
    geom_bar(show.legend = FALSE, stat = "identity") +
    xlab("Country") +
    ylab("VSI") + 
    ggtitle("Vaccination Success Index score") +
    theme_bw() +
    theme(axis.text.x = element_text(angle = 60, hjust=1))

ggplot(new.data.with.VSI, aes(x = reorder(country, -VSI), y = VSI, fill = country)) + 
  geom_bar(stat = "identity")

```

We can see that Somalia appears to be the country with the lowest VSI, and Cuba appears to have the highest VSI. 
After the merge of VSI with available world sustainability and cases data (`wsd.covid.all`), we end up with 99 countries for whom we have both information.

Let's now see how socioeconomic factors may be associated with VSI. To develop the most accurate prediction, we will compare a range of methods going from supervised to unsupervised:

* Linear models
* LASSO model selection
* random forests
* neural networks?

### Linear models

First, we try a few simple linear models by selecting variables based on domain knowledge. We try the same models on wsd data from 2018, wsd over the last 3 years, wsd average and wsd change between 2001 and 2018 to see which data best explains VSI. 


#### WSD 2018

```{r fig.width=10, fig.height=10}

# latest wsd data (from 2018)
# merge VSI to the wsd data
wsd.2018.vsi <- inner_join(new.data.with.VSI[c("country", "VSI")], wsd.2018, by = "country")
nrow(wsd.2018.vsi) # countries left after merge

# create correlation plot to see whether some variables are highly correlated (not tolerated by lm)
wsd.2018.vsi %>% 
  select(-VSI) %>% # remove outcome variable as we want to highlight corrs among predictors only
  select_if(., is.numeric) %>%
  cor(.) %>%
  # corrplot(method = 'color', order = 'alphabet', type = 'lower', diag = FALSE)
  corrplot(method = "color", 
           type = "upper", order = "hclust", 
           addCoef.col = "black", # Add coefficient of correlation
           tl.col="black", tl.srt = 45, #Text label color and rotation
           # hide correlation coefficient on the principal diagonal
           diag = FALSE 
           )
# find names of highly correlated variables
correlations <- wsd.2018.vsi %>% 
  select(-VSI) %>%
  select_if(., is.numeric) %>%
  cor(.)
correlations[correlations < 0.8 | correlations ==1] <- ""
correlations

# some variables are perfectly or almost perfectly correlated. We will trim variables that have correlations over 0.8 to avoid multi-collinearity. 
wsd.2018.vsi <- wsd.2018.vsi %>%
  select(-c(country, Country.Code, Continent, WorldRegion_2018, # also dropping country/continent information 
            Electricity.Access_2018, # LifeExpenctancy_2018 corr 0.83
            TotalNaturalResources.GDP_2018, # SavingNaturalResourceDepletion_2018 corr 0.86
            ImportGoodsServices.GDP_2018, # ExportGoodsServices.GDP_2018 corr 0.92
            Trade.GDP_2018, # ExportGoodsServices.GDP_2018 0.98
            GrossNationalExpenditure.GDP_2018, # FinalConsumptionExpenditure.GDP_2018 corr 0.88
            RuralPopulation.Prop_2018 # UrbanPopulation.Prop_2018 corr 1
            ))


# # trying a few models with variables that should make sense...
# internet use
lm.2018.1 <- lm(VSI ~ IndividualsUsingInternet_2018, data = wsd.2018.vsi)
summary(lm.2018.1)
# internet use does explain VSI when life expectancy and other vars are not there

# regime type
lm.2018.2 <- lm(VSI ~ RegimeType_2018, data = wsd.2018.vsi)
summary(lm.2018.2)
# regime type also significantly impacts VSI when used alone

# urban population percentage
lm.2018.3 <- lm(VSI ~ UrbanPopulation.Prop_2018, data = wsd.2018.vsi)
summary(lm.2018.3)
# urban population also significantly impacts VSI when used alone

# years of compulsory education
lm.2018.4 <- lm(VSI ~ CompulsoryEducationDurationYears_2018, data = wsd.2018.vsi)
summary(lm.2018.4)
# compulsory education also significantly impacts VSI when used alone

# GDP per capita
lm.2018.5 <- lm(VSI ~ GDP.PerCapita.Current_2018, data = wsd.2018.vsi)
summary(lm.2018.5)
# GDP per capita also significantly impacts VSI when used alone

# Life expectancy
lm.2018.6 <- lm(VSI ~ LifeExpenctancy_2018, data = wsd.2018.vsi)
summary(lm.2018.6)
# Life expectancy also significantly impacts VSI when used alone

# model with all these variables
lm.2018.7 <- lm(VSI ~ LifeExpenctancy_2018 + 
                  UrbanPopulation.Prop_2018 + 
                  RegimeType_2018 + 
                  CompulsoryEducationDurationYears_2018 + 
                  GDP.PerCapita.Current_2018 +
                  IndividualsUsingInternet_2018, 
               data = wsd.2018.vsi)
summary(lm.2018.7)
# only Life Expectancy is useful here, It seems like this variable may be sufficient at predicting VSI. 

# fit model (without any country/continent info)
lm.2018.full <- lm(VSI ~ ., data = wsd.2018.vsi)
summary(lm.2018.full)

# only LifeExpenctancy_2018 shows a significant effect on VSI. Trying a reduced model with only this variable
lm.2018.red <- lm(VSI ~ LifeExpenctancy_2018, data = wsd.2018.vsi)
summary(lm.2018.red)
anova(lm.2018.full, lm.2018.red)

```

There is no evidence to suggest that the full model better explains the data than the reduced model. It seems like life expectancy alone is sufficient for explaining VSI. 

#### 3 year average

```{r fig.width=10, fig.height=10}
# wsd average over the past 3 years
# merge VSI to the wsd data
wsd.3yr.vsi <- inner_join(new.data.with.VSI[c("country", "VSI")], wsd.3yr, by = "country")
nrow(wsd.3yr.vsi) # countries left after merge

# create correlation plot to see whether some variables are highly correlated (not tolerated by lm)
wsd.3yr.vsi %>% 
  select(-VSI) %>% # remove outcome variable as we want to highlight corrs among predictors only
  select_if(., is.numeric) %>%
  cor(.) %>%
  # corrplot(method = 'color', order = 'alphabet', type = 'lower', diag = FALSE)
  corrplot(method = "color", 
           type = "upper", order = "hclust", 
           addCoef.col = "black", # Add coefficient of correlation
           tl.col="black", tl.srt = 45, #Text label color and rotation
           # hide correlation coefficient on the principal diagonal
           diag = FALSE 
           )
# find names of highly correlated variables
correlations <- wsd.3yr.vsi %>% 
  select(-VSI) %>%
  select_if(., is.numeric) %>%
  cor(.)
correlations[correlations < 0.8 | correlations ==1] <- ""
correlations

# some variables (same as 2018) are perfectly or almost perfectly correlated. We will trim variables that have correlations over 0.8 to avoid multi-collinearity. 
wsd.3yr.vsi <- wsd.3yr.vsi %>%
  select(-c(country, WorldRegion_3yr, # also dropping country/continent information 
            Electricity.Access_3yr, # LifeExpenctancy_3yr corr 0.83
            TotalNaturalResources.GDP_3yr, # SavingNaturalResourceDepletion_3yr corr 0.86
            ImportGoodsServices.GDP_3yr, # ExportGoodsServices.GDP_23yr corr 0.92
            Trade.GDP_3yr, # ExportGoodsServices.GDP_3yr 0.98
            GrossNationalExpenditure.GDP_3yr, # FinalConsumptionExpenditure.GDP_3yr corr 0.88
            RuralPopulation.Prop_3yr # UrbanPopulation.Prop_2018 corr 1
            ))

# # trying a few models with variables that should make sense...
# internet use
lm.3yr.1 <- lm(VSI ~ IndividualsUsingInternet_3yr, data = wsd.3yr.vsi)
summary(lm.3yr.1)
# internet use does explain VSI when life expectancy and other vars are not there

# regime type
lm.3yr.2 <- lm(VSI ~ RegimeType_3yr, data = wsd.3yr.vsi)
summary(lm.3yr.2)
# regime type also significantly impacts VSI when used alone

# urban population percentage
lm.3yr.3 <- lm(VSI ~ UrbanPopulation.Prop_3yr, data = wsd.3yr.vsi)
summary(lm.3yr.3)
# urban population also significantly impacts VSI when used alone

# years of compulsory education
lm.3yr.4 <- lm(VSI ~ CompulsoryEducationDurationYears_3yr, data = wsd.3yr.vsi)
summary(lm.3yr.4)
# compulsory education also significantly impacts VSI when used alone

# GDP per capita
lm.3yr.5 <- lm(VSI ~ GDP.PerCapita.Current_3yr, data = wsd.3yr.vsi)
summary(lm.3yr.5)
# GDP per capita also significantly impacts VSI when used alone

# Life expectancy
lm.3yr.6 <- lm(VSI ~ LifeExpenctancy_3yr, data = wsd.3yr.vsi)
summary(lm.3yr.6)
# Life expectancy also significantly impacts VSI when used alone

# model with all these variables
lm.3yr.7 <- lm(VSI ~ LifeExpenctancy_3yr + 
                  UrbanPopulation.Prop_3yr + 
                  RegimeType_3yr + 
                  CompulsoryEducationDurationYears_3yr + 
                  GDP.PerCapita.Current_3yr +
                  IndividualsUsingInternet_3yr, 
               data = wsd.3yr.vsi)
summary(lm.3yr.7)
# only Life Expectancy is useful here, It seems like this variable may be sufficient at predicting VSI. 

# fit model (without any country/continent info)
lm.3yr.full <- lm(VSI ~ ., data = wsd.3yr.vsi)
summary(lm.3yr.full)

# only LifeExpenctancy_2018 shows a significant effect on VSI. Trying a reduced model with only this variable
lm.3yr.red <- lm(VSI ~ LifeExpenctancy_3yr, data = wsd.3yr.vsi)
summary(lm.3yr.red)
anova(lm.3yr.full, lm.3yr.red)


```
Same conclusion here: life expectancy is the only variable that significantly impacts VSI. 

#### 17 year average

```{r fig.width=10, fig.height=10}
# wsd average over all years (2001-2018)
# merge VSI to the wsd data
wsd.avg.vsi <- inner_join(new.data.with.VSI[c("country", "VSI")], wsd.avg, by = "country")
nrow(wsd.avg.vsi) # countries left after merge

# create correlation plot to see whether some variables are highly correlated (not tolerated by lm)
wsd.avg.vsi %>% 
  # select(-VSI) %>% # remove outcome variable as we want to highlight corrs among predictors only
  select_if(., is.numeric) %>%
  cor(.) %>%
  # corrplot(method = 'color', order = 'alphabet', type = 'lower', diag = FALSE)
  corrplot(method = "color", 
           type = "upper", order = "hclust", 
           addCoef.col = "black", # Add coefficient of correlation
           tl.col="black", tl.srt = 45, #Text label color and rotation
           # hide correlation coefficient on the principal diagonal
           diag = FALSE 
           )
# find names of highly correlated variables
correlations <- wsd.avg.vsi %>% 
  select(-VSI) %>%
  select_if(., is.numeric) %>%
  cor(.)
correlations[correlations < 0.8 | correlations ==1] <- ""
correlations

# some variables (same as 2018 + 1 variable, GDP per capita) are perfectly or almost perfectly correlated. We will trim variables that have correlations over 0.8 to avoid multi-collinearity. 
wsd.avg.vsi <- wsd.avg.vsi %>%
  select(-c(country, WorldRegion_avg, # also dropping country/continent information 
            Electricity.Access_avg, # LifeExpenctancy_avg corr 0.83
            TotalNaturalResources.GDP_avg, # SavingNaturalResourceDepletion_avg corr 0.86
            ImportGoodsServices.GDP_avg, # ExportGoodsServices.GDP_2avg corr 0.92
            Trade.GDP_avg, # ExportGoodsServices.GDP_avg 0.98
            GrossNationalExpenditure.GDP_avg, # FinalConsumptionExpenditure.GDP_avg corr 0.88
            RuralPopulation.Prop_avg, # UrbanPopulation.Prop_2018 corr 1
            GDP.PerCapita.Current_avg # IndividualsUsingInternet_avg corr 0.84
            ))

# # trying a few models with variables that should make sense...
# internet use
lm.avg.1 <- lm(VSI ~ IndividualsUsingInternet_avg, data = wsd.avg.vsi)
summary(lm.avg.1)
# internet use does explain VSI when life expectancy and other vars are not there

# regime type
lm.avg.2 <- lm(VSI ~ RegimeType_avg, data = wsd.avg.vsi)
summary(lm.avg.2)
# regime type also significantly impacts VSI when used alone

# urban population percentage
lm.avg.3 <- lm(VSI ~ UrbanPopulation.Prop_avg, data = wsd.avg.vsi)
summary(lm.avg.3)
# urban population also significantly impacts VSI when used alone

# years of compulsory education
lm.avg.4 <- lm(VSI ~ CompulsoryEducationDurationYears_avg, data = wsd.avg.vsi)
summary(lm.avg.4)
# compulsory education also significantly impacts VSI when used alone

# GDP 
lm.avg.5 <- lm(VSI ~ GDP.Current_avg, data = wsd.avg.vsi)
summary(lm.avg.5)
# GDP also significantly impacts VSI when used alone

# Life expectancy
lm.avg.6 <- lm(VSI ~ LifeExpenctancy_avg, data = wsd.avg.vsi)
summary(lm.avg.6)
# Life expectancy also significantly impacts VSI when used alone

# model with all these variables
lm.avg.7 <- lm(VSI ~ LifeExpenctancy_avg + 
                  UrbanPopulation.Prop_avg + 
                  RegimeType_avg + 
                  CompulsoryEducationDurationYears_avg + 
                  GDP.Current_avg +
                  IndividualsUsingInternet_avg, 
               data = wsd.avg.vsi)
summary(lm.avg.7)
# only Life Expectancy is useful here, It seems like this variable may be sufficient at predicting VSI. 

# fit model (without any country/continent info)
lm.avg.full <- lm(VSI ~ ., data = wsd.avg.vsi)
summary(lm.avg.full)

# only LifeExpenctancy_2018 shows a significant effect on VSI. Trying a reduced model with only this variable
lm.avg.red <- lm(VSI ~ LifeExpenctancy_avg + 
                   RegimeType_avg + 
                   Adj.SavingNaturalResourceDepletion_avg +
                   Adj.SavingNetForestDepletion_avg , data = wsd.avg.vsi)
summary(lm.avg.red)

# let's further reduce it to keep only significant variables
lm.avg.red <- lm(VSI ~ LifeExpenctancy_avg + RegimeType_avg , data = wsd.avg.vsi)
summary(lm.avg.red)
anova(lm.avg.full, lm.avg.red)

```

Interesting! In the average over ~20 years (2001-2018), regime type (electoral autocracy, electoral democracy), SavingNaturalResourceDepletion_avg,  SavingNetForestDepletion_avg and LifeExpenctancy_avg now have become significant. The reduced model suggests that out of these, LifeExpenctancy_avg and Regime_Type are sufficient. 

####17 year change

```{r fig.width=10, fig.height=10}
# wsd change between 2001 and 2018
# merge VSI to the wsd data
wsd.diff.vsi <- inner_join(new.data.with.VSI[c("country", "VSI")], wsd.change, by = "country")
nrow(wsd.diff.vsi) # countries left after merge

# create correlation plot to see whether some variables are highly correlated (not tolerated by lm)
wsd.change.vsi %>% 
  select(-VSI) %>% # remove outcome variable as we want to highlight corrs among predictors only
  select_if(., is.numeric) %>%
  cor(.) %>%
  # corrplot(method = 'color', order = 'alphabet', type = 'lower', diag = FALSE)
  corrplot(method = "color", 
           type = "upper", order = "hclust", 
           addCoef.col = "black", # Add coefficient of correlation
           tl.col="black", tl.srt = 45, #Text label color and rotation
           # hide correlation coefficient on the principal diagonal
           diag = FALSE 
           )
# find names of highly correlated variables
correlations <- wsd.change.vsi %>% 
  select(-VSI) %>%
  select_if(., is.numeric) %>%
  cor(.)
correlations[correlations < 0.8 | correlations ==1] <- ""
correlations

# some variables (same as 2018 + 1 variable, GDP per capita) are perfectly or almost perfectly correlated. We will trim variables that have correlations over 0.8 to avoid multi-collinearity. 
wsd.diff.vsi <- wsd.diff.vsi %>%
  select(-c(country, # also dropping country/continent information 
            Electricity.Access_diff, # LifeExpenctancy_diff corr 0.83
            TotalNaturalResources.GDP_rel.diff, # SavingNaturalResourceDepletion_diff corr 0.86
            ImportGoodsServices.GDP_rel.diff, # ExportGoodsServices.GDP_2diff corr 0.92
            Trade.GDP_rel.diff, # ExportGoodsServices.GDP_diff 0.98
            GrossNationalExpenditure.GDP_diff, # FinalConsumptionExpenditure.GDP_diff corr 0.88
            RuralPopulation.Prop_diff, # UrbanPopulation.Prop_2018 corr 1
            GDP.PerCapita.Current_diff, # IndividualsUsingInternet_diff corr 0.84
            GDP.Current_rel.diff, # GDP.PerCapita.Current_rel.diff corr 0.96
            CompulsoryEducationDurationYears_rel.diff, # CompulsoryEducationDurationYears_diff corr 0.99
            ExportGoodsServices.GDP_rel.diff, # ExportGoodsServices.GDP_diff corr 0.84
            FinalConsumptionExpenditure.GDP_rel.diff, # FinalConsumptionExpenditure.GDP_diff corr 0.94
            Govt.FinalConsumptionExpenditure.GDP_rel.diff, # Govt.FinalConsumptionExpenditure.GDP_diff 0.95
            GrossNationalExpenditure.GDP_rel.diff, # GrossNationalExpenditure.GDP_diff corr 0.98
            WomenInBusinessLawIndex_rel.diff, # WomenInBusinessLawIndex_diff corr 0.9
            ))

# # trying a few models with variables that should make sense...
# internet use
lm.diff.1 <- lm(VSI ~ IndividualsUsingInternet_diff, data = wsd.diff.vsi)
summary(lm.diff.1)
# internet use does not explain VSI 

# regime type
lm.diff.2 <- lm(VSI ~ RegimeType_diff, data = wsd.diff.vsi)
summary(lm.diff.2)
# regime type does not impact VSI when used alone

# urban population percentage
lm.diff.3 <- lm(VSI ~ UrbanPopulation.Prop_diff, data = wsd.diff.vsi)
summary(lm.diff.3)
# urban population does not impact VSI when used alone

# years of compulsory education
lm.diff.4 <- lm(VSI ~ CompulsoryEducationDurationYears_diff, data = wsd.diff.vsi)
summary(lm.diff.4)
# compulsory education does not VSI when used alone

# GDP 
lm.diff.5 <- lm(VSI ~ GDP.Current_diff, data = wsd.diff.vsi)
summary(lm.diff.5)
# GDP does not impact VSI when used alone

# Life expectancy
lm.diff.6 <- lm(VSI ~ LifeExpenctancy_diff, data = wsd.diff.vsi)
summary(lm.diff.6)
# Life expectancy does not impact VSI when used alone

# model with all these variables
lm.diff.7 <- lm(VSI ~ LifeExpenctancy_diff + 
                  UrbanPopulation.Prop_diff + 
                  RegimeType_diff + 
                  CompulsoryEducationDurationYears_diff + 
                  GDP.Current_diff +
                  IndividualsUsingInternet_diff, 
               data = wsd.diff.vsi)
summary(lm.diff.7)
# only Life Expectancy is useful here, It seems like this variable may be sufficient at predicting VSI. 

# fit model (without any country/continent info)
# lm.diff.full <- lm(VSI ~ ., data = wsd.diff.vsi) # coefficients not defined due to singularities. Outputs NAs
# summary(lm.diff.full)
```


```{r}
# Split the data 
N <- length(df$country)
n1 <- floor(.6*N)
n2 <- floor(.2*N)

#dropping country, continent and country.code 
df.pred <- df %>% dplyr::select(-c(country, Continent, Country.Code))
  
set.seed(10)

# Split data to three portions of .6, .2 and .2 of data size N
idx_train <- sample(N, n1)
idx_no_train <- (which(! seq(1:N) %in% idx_train))
idx_test <- sample(idx_no_train, n2)
idx_val <- which(! idx_no_train %in% idx_test)
data.train <- as.data.frame(df.pred[idx_train,])
data.test <- as.data.frame(df.pred[idx_test,])
data.val <- as.data.frame(df.pred[idx_val,])
data.train.test <- as.data.frame(df.pred[-idx_val,]) #for methods that don't require separate test set
```

Here's code to subdivide the train/test dfs based on what type of WSD predictors they include (i.e. so you can easily train a model only on the WSD 17-yr averages, plus static measures and covid cases/deaths). We should also pair down on the covid case/death variables in here!

```{r}
# WSD 2018
data.train_2018 <- data.train %>% dplyr::select(-c(contains("_3yr") | contains("_avg") | contains("_rel.diff") | contains("_diff")))
data.test_2018 <- data.test %>% dplyr::select(-c(contains("_3yr") | contains("_avg") | contains("_rel.diff") | contains("_diff")))
data.val_2018 <- data.val %>% dplyr::select(-c(contains("_3yr") | contains("_avg") | contains("_rel.diff") | contains("_diff")))
data.train.test_2018 <- data.train.test %>% dplyr::select(-c(contains("_3yr") | contains("_avg") | contains("_rel.diff") | contains("_diff")))

# 3 year average
data.train_3yr <- data.train %>% dplyr::select(-c(contains("_2018") | contains("_avg") | contains("_rel.diff") | contains("_diff")))
data.test_3yr <- data.test %>% dplyr::select(-c(contains("_2018") | contains("_avg") | contains("_rel.diff") | contains("_diff")))
data.val_3yr <- data.val %>% dplyr::select(-c(contains("_2018") | contains("_avg") | contains("_rel.diff") | contains("_diff")))
data.train.test_3yr <- data.train.test %>% dplyr::select(-c(contains("_2018") | contains("_avg") | contains("_rel.diff") | contains("_diff")))

# 17 year average
data.train_avg <- data.train %>% dplyr::select(-c(contains("_2018") | contains("_3yr") | contains("_rel.diff") | contains("_diff")))
data.test_avg <- data.test %>% dplyr::select(-c(contains("_2018") | contains("_3yr") | contains("_rel.diff") | contains("_diff")))
data.val_avg <- data.val %>% dplyr::select(-c(contains("_2018") | contains("_3yr") | contains("_rel.diff") | contains("_diff")))
data.train.test_avg <- data.train.test %>% dplyr::select(-c(contains("_2018") | contains("_3yr") | contains("_rel.diff") | contains("_diff")))

#17 year change
data.train_diff <- data.train %>% dplyr::select(-c(contains("_2018") | contains("_3yr") | contains("_avg")))
data.test_diff <- data.test %>% dplyr::select(-c(contains("_2018") | contains("_3yr") | contains("_avg")))
data.val_diff <- data.val %>% dplyr::select(-c(contains("_2018") | contains("_3yr") | contains("_avg")))
data.train.test_diff <- data.train.test %>% dplyr::select(-c(contains("_2018") | contains("_3yr") | contains("_avg")))
```

The 17 year change variables seem overall less useful in predicting VSI. 

### LASSO model selection

Let's now try a slightly more data-driven approach, i.e. using LASSO to select the most relevant variables. Using the `wsd.avg.vsi` 17 year average dataset.
Could also potentially use `wsd.sum.wide` here that has all 4 types of variables (2018, 3y average, 17y average, 17y difference), but that would mean dropping some variables first as LASSO doesn't like NAs. 

```{r}

# remove all rows containing NAs (LASSO does not work without this step) - no NAs here it seems
wsd.avg.vsi.sub <- na.omit(wsd.avg.vsi)
nrow(wsd.avg.vsi.sub)

# create matrices to feed into gmnet
Y <- as.matrix(wsd.avg.vsi.sub[, 1]) # extract Y (VSI)
X <- model.matrix(VSI~., data = wsd.avg.vsi.sub)[, -1] # remove the first (interecept) column with only 1s

# to control the randomness in K folds 
set.seed(10)  

# run LASSO
lasso.avg <- cv.glmnet(X, Y, alpha = 1, nfolds = 10, intercept = TRUE)  

# plot LASSO results
plot(lasso.avg)

lasso.avg$lambda.1se # use lambda.1se to select the smaller model
coef.1se <- coef(lasso.avg, s = "lambda.1se") # get coefficients
coef.1se <- coef.1se[which(coef.1se != 0),] # show variables with non-zero coefficients

# output variable names
coef.1se <- rownames(as.matrix(coef.1se))[-1] 
coef.1se

# output lm based on LASSO
coef.1se <- coef(lasso.avg, s="lambda.1se")  #s=c("lambda.1se","lambda.min") or lambda value
coef.1se <- coef.1se[which(coef.1se != 0),]   # get the non=zero coefficients
var.1se <- rownames(as.matrix(coef.1se))[-1] # output the variable names without intercept
var.1se <- var.1se[-grep("RegimeType_avg", var.1se)] # remove the state levels
wsd.avg.vsi.lasso <-  wsd.avg.vsi.sub %>%
  select(c("RegimeType_avg", "VSI", var.1se)) # get a subset with response and LASSO output

# run relaxed LASSO
fit.1se.lm <- lm(VSI~., data = wsd.avg.vsi.lasso) 
summary(fit.1se.lm) 

```

LASSO results using the `wsd.avg.vsi` suggest the same variables found with linear models. 


