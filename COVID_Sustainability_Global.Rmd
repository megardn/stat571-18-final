---
title: "COVID_Sustainability_Global"
author:
- Diego G. Davila
- Margaret Gardner
- Joelle Bagautdinova
date: 'Due before midnight, May 1st'
output:
  html_document:
    code_folding: show
    highlight: haddock
    theme: lumen
    toc: yes
    toc_depth: 4
    toc_float: yes
    number_sections: yes
  pdf_document:
    toc_depth: '4'
    number_sections: yes
urlcolor: blue
---

```{r Setup, include=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width=8, fig.height=4)
options(scipen = 0, digits = 3)  # controls base R output

# Package setup
if(!require("pacman")) install.packages("pacman")

pacman::p_load(tidyverse, dplyr, ggplot2, ggthemes, data.table, lubridate,
               GGally, RColorBrewer, ggsci, plotROC, usmap,
               plotly, ggpubr, vistime, skimr, glmnet, dgCMatrix, leaps, car, keras, neuralnet, tensorflow, missForest)
```



Load outputs from EDA Rmds
```{r load.dfs}
load("data/WorldSustainabilityData_Processed.Rda")
#skim(wsd)
max(wsd$Year)
wsd <- wsd %>% dplyr::select(1:3,"Continent",4:21,23:ncol(wsd)) %>% # rearrange a little to group continent with the other geographic data
  mutate(Country.Name=as.factor(Country.Name)) %>%
  rename(country=Country.Name)

load("data/covid_cases_vaccines_clean.Rda")
#skim(covid)
```

```{r}
#some discrepancy in how countries are named, gonna fix that
wsd.country <- levels(wsd$country)
covid.country <- levels(covid$country)
#setdiff(wsd.country, covid.country) #only in wsd, not covid
#setdiff(covid.country, wsd.country) #only in covid, not wsd

covid$country <- recode_factor(covid$country, "UK"="United Kingdom", "Gambia"="Gambia, The", "Laos"="Lao PDR", "Egypt"="Egypt, Arab Rep.", "South Korea"="Korea, Rep.", "USA"="United States", "Russia"="Russian Federation", "Venezuela"="Venezuela, RB", "Democratic Republic Of The Congo"="DRC")
wsd$country <- recode_factor(wsd$country,  "Iran, Islamic Rep."="Iran", "Bosnia and Herzegovina"="Bosnia And Herzegovina", "Cote d'Ivoire"="Cote D Ivoire", "Guinea-Bissau"="Guinea Bissau", "Kyrgyz Republic"="Kyrgyzstan", "North Macedonia"="Macedonia", "Eswatini"="Swaziland", "Vietnam"="Viet Nam", "Timor-Leste"="Timor Leste", "Slovak Republic"="Slovakia", "Congo, Rep."="Congo", "Congo, Dem. Rep."="DRC")
```

Before attempting to merge, I'll parse down the dataframes so we can hopefully we can mix and match as needed to get what we need for our final analyses.

## WSD Summary Vales

To start, I'll make a wide df the WSD data into the COVID dataset containing: the latest (2018) data; average of each parameter over time (2001 to 2018); the average of the last 3 years of data (2016-2018 for most countries); change in each parameter over time (2001-2018).

``` {r}
# 2018 values
wsd.2018 <- wsd %>% subset(Year==max(wsd$Year)) %>% dplyr::select(-Year) #filter to last year of WSD data
colnames(wsd.2018)[4:30] <- paste(colnames(wsd.2018)[4:30], "2018", sep = "_") #add suffix
head(wsd.2018)
n_unique(wsd.2018$country) #only 109 of 141 countries have 2018 data
```
Because only `r n_unique(wsd.2018$country)` of `r n_unique(wsd$country)` countries have sustainability data in 2018, I've also made a df including the latest available data from each country. However, we may not want to use it since the last data represents a wide range from `r unname(summary(wsd.latest$Year_latest)[1])` to `unname(summary(wsd.latest$Year_latest)[6])`

```{r}
# latest data for each country
wsd.latest <- wsd %>% arrange(Year) %>%
  group_by(country) %>%
  slice_tail(n=1) %>%
  ungroup()
colnames(wsd.latest)[2:31] <- paste(colnames(wsd.latest)[2:31], "latest", sep = "_") #add suffix
table(as.factor(wsd.latest$Year_latest)) # see yr frequencies
```

Values averaged across 2016-2018; only includes countries with data from all 3 years. 

```{r}
# 3-yr averages for numeric
wsd.3yr.num <- wsd %>% arrange(Year) %>%
  filter(Year >= 2016) %>% #get 2016-2018 data
  group_by(country) %>%
  filter(n() == 3) %>%
  summarise_if(is.numeric, mean) %>% #means
  ungroup()
colnames(wsd.3yr.num)[2:26] <- paste(colnames(wsd.3yr.num)[2:26], "3yr", sep = "_") #add suffix


# 3-yr mode for factors
# create a function to find the mode. Taken from: https://stackoverflow.com/questions/2547402/how-to-find-the-statistical-mode
Mode <- function(x) {
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}

wsd.3yr.fac <- wsd %>% arrange(Year) %>%
  group_by(country) %>%
  filter(Year >= 2016) %>% #get 2016-2018 data
  filter(n() == 3) %>%
  mutate(WorldBankIncomeClass = Mode(WorldBankIncomeClass),
         RegimeType = Mode(RegimeType),
         WorldRegion=Mode(WorldRegion),
         nYears=n()) %>% #count how many years are being averaged for each country) %>%
  ungroup() %>%
  dplyr::select(country, nYears, WorldBankIncomeClass, RegimeType, WorldRegion) %>%
  distinct()
#n_unique(wsd.3yr.fac$country) #make sure no countries missing
colnames(wsd.3yr.fac)[2:5] <- paste(colnames(wsd.3yr.fac)[2:5], "3yr", sep = "_") #add suffix

wsd.3yr <- merge(wsd.3yr.fac, wsd.3yr.num, by="country") %>% #rejoin num and fac
    dplyr::select(-Year_3yr, -nYears_3yr)
nrow(wsd.3yr)
```

Averaging across all years for every country with available data.  **add table/graph to show how many years each country has data for? or is this more of an EDA thing? Should EDA be it's own section above the Summary Values ones or should it all be presented together?**

```{r}
# averages for numeric
wsd.avg.num <- wsd %>% arrange(Year) %>%
  group_by(country) %>%
  summarise_if(is.numeric, mean) %>%
  ungroup() %>%
  dplyr::select(-Year)
colnames(wsd.avg.num)[2:25] <- paste(colnames(wsd.avg.num)[2:25], "avg", sep = "_") #add suffix

# mode for factors
wsd.avg.fac <- wsd %>% arrange(Year) %>%
  group_by(country) %>%
  mutate(WorldBankIncomeClass = Mode(WorldBankIncomeClass),
         RegimeType = Mode(RegimeType),
         WorldRegion=Mode(WorldRegion),
         nYears=n()) %>% #count how many years are being averaged for each country
  ungroup() %>%
  dplyr::select(country, WorldBankIncomeClass, RegimeType, WorldRegion, nYears) %>%
  distinct()
colnames(wsd.avg.fac)[2:5] <- paste(colnames(wsd.avg.fac)[2:5], "avg", sep = "_") #add suffix

# #make sure modes returned as expected 
# angola <- wsd$WorldBankIncomeClass[wsd$country=="Angola"]
# Mode(angola)
# wsd.avg.fac$WorldBankIncomeClass[wsd.avg.fac$country=="Angola"]
# #looks good!

wsd.avg <- merge(wsd.avg.fac, wsd.avg.num, by="country") #rejoin num and fac
table(as.factor(wsd.avg$nYears_avg)) # see yr frequencies
```

As you can see from the frequency table above, only 60 countries have data for all 18 years. We could restrict this to only including the averages of countries with at least 15 yrs of data by uncommenting the chunk below (filters df down to 99 countries):

```{r}
# #uncomment to remove any countries that have less than 15 years worth of data
# wsd.avg <- wsd.avg %>%
#   subset(nYears_avg >= 15)
# nrow(wsd.avg)
```

Change in each factor over 10 years (2008-2018).

```{r, warning = FALSE}
# change in each parameter over 10 years
# define function to get relative change
rel.diff <- function(x) {(x - lag(x))/lag(x)} # define function to get relative change

wsd.change.num <- wsd %>% 
  group_by(country) %>%
  arrange(Year) %>%
  select_if(is.numeric)%>% # just dealing with numeric for now
  subset(Year==2018 | Year == 2008) %>%
  filter(n() >= 2) %>% #make sure country has data for both years
  mutate(across(where(is.numeric), ~rel.diff(.), .names="{.col}_rel.diff")) %>% # change normalized by baseline values
  mutate(across(c(!ends_with("diff")), ~diff(.x), .names="{.col}_diff"))  %>%  #absolute change 
  mutate_if(is.numeric, function(x) ifelse(is.infinite(x), 0, x)) %>% # change Inf values to 0
  dplyr::select(-Year, -Year_diff, -Year_rel.diff) %>%
  drop_na() %>%
  ungroup()

#now factors
wsd.change.fac <- wsd %>%
  dplyr::select(country, Year, WorldBankIncomeClass, RegimeType)
# order factors
wsd.change.fac$WorldBankIncomeClass <- factor(wsd.change.fac$WorldBankIncomeClass, levels=c("Low income", "Lower-middle income", "Upper-middle income", "High income"), ordered=TRUE)
wsd.change.fac$RegimeType <- factor(wsd.change.fac$RegimeType, levels=c("Closed Autocracy", "Electoral Autocracy", "Electoral Democracy", "Liberal Democracy"), ordered=TRUE)

wsd.change.fac <- wsd.change.fac %>% 
  group_by(country) %>%
  arrange(Year) %>%
  subset(Year==2018 | Year == 2008) %>%
  filter(n() >= 2) %>% #make sure country has data for both years
  mutate(WorldBankIncomeClass_diff = unclass(WorldBankIncomeClass) - unclass(lag(WorldBankIncomeClass)),
         WorldBankIncomeClass_rel.diff = rel.diff(unclass(WorldBankIncomeClass)),
         RegimeType_diff = unclass(RegimeType) - unclass(lag(RegimeType)),
         RegimeType_rel.diff = rel.diff(unclass(RegimeType))) %>% 
  mutate_if(is.numeric, function(x) ifelse(is.infinite(x), 0, x)) %>% # change Inf values to 0
  dplyr::select(-Year) %>%
  drop_na() %>%
  ungroup()

#now join and drop unnecessary columns used for calculations
wsd.change <- merge(wsd.change.num, wsd.change.fac, by ="country") %>%
  dplyr::select(country, ends_with("diff"))
```

Merge into one wide wsd dataframe:

```{r}
# one predictor dataframe to rule them all
wsd.df_list <- list(wsd.2018, wsd.3yr, wsd.avg, wsd.change) #dfs to merge
wsd.sum.wide <- wsd.df_list %>% reduce(full_join, by='country') #join
#skim(wsd.sum.wide)

#here's a long version if you want it
wsd.sum.long <- wsd.sum.wide %>%
  dplyr::select(-WorldBankIncomeClass_rel.diff, -RegimeType_rel.diff) %>%
  mutate(WorldBankIncomeClass_diff=as.factor(WorldBankIncomeClass_diff),
         RegimeType_diff=as.factor(RegimeType_diff)) %>%
  pivot_longer(c(contains("_")), 
    names_to = c(".value", "type"), 
    names_sep = "_", 
    values_drop_na = TRUE
  )

head(wsd.sum.wide)
```

## COVID Summary Values

In order to come up with meaningful summary predictors and dependent variables that are compatible with our WSD, we'll next summaries our data on covid cases, deaths, and vaccinations. For our purposes in this project, we'll treat cases and deaths prior to each country's vaccine roll-out as independent variables (predictors) alongside the WSD. Vaccination data will be synthesized using PCA to create a the Vaccination Success Index (VSI), a composite score which we will use to indicate how well a country was able to vaccinate its citizens. 

First, we create a dataframe with the latest COVID data from each country. Every country has some data from March 17, 2022, though some countries' last reported vaccination data was from as early as `unname(summary(last.timepoint$date.vac_last)[1])`.

```{r}
#last day with data for each country
last.timepoint1 <- covid %>% group_by(country) %>%
  arrange(country,date) %>%
  slice_tail(n=1) %>%
  ungroup()
last.timepoint_filt <- last.timepoint1[, which(colMeans(!is.na(last.timepoint1)) > 0.5)] #remove cols with NA > 50%
#skim(last.timepoint_filt)

# get missing column names
missing_vac_cols <-  names(which(colMeans(is.na(last.timepoint1)) > 0.5))

# add last reported (within 2022) vaccination info
vac_info_2022 <- covid %>%
  drop_na(people_vaccinated_per_hundred) %>%   # filter to keep only non-NA in people_vaccinated_per_hundred
  group_by(country) %>% 
  filter(date == max(date)) %>%   # keep last reported date for every country
  filter(year == 2022) %>% # keep only countries who last reported in 2022 (as we are interested in a recent vaccination status)
  # filter(month_year == "2022-03-01") # could also filter for march 2022 specifically
  dplyr::select(country, date, missing_vac_cols) %>% # keep only variables not yet included in last.timepoint
  rename(date.vac = date)# rename the date to correspond to last reported vaccine info

# merge the last reported vaccination info with last reported cases/deaths info
last.timepoint <- merge(last.timepoint_filt, vac_info_2022, by = "country")
colnames(last.timepoint)[2:43] <- paste(colnames(last.timepoint)[2:43], "last", sep = "_") #add suffix
#names(last.timepoint)

# #find date of vaccination events
# table(covid$vaccination_events)
# table(covid$vaccine_dose_events)

summary(last.timepoint$date.vac_last)
```

Next, we'll calculate predictors from COVID case and death rates prior to the day each country reported starting to vaccinate. 

``` {r, warning=FALSE}
# find the date each country reported starting to vaccinate
vac_start_df <- covid %>%
  dplyr::select(country, date, vaccination_events) %>%
  filter(vaccination_events == "vac_start") %>%
  mutate(days.to.start_rel = as.numeric(difftime(date, min(date), units = "days")),
         days.to.start_abs = as.numeric(difftime(date, "2020-12-01", units = "days"))) %>%
  rename(date.first.vac = date)
# adding date of first vaccination for each country back to the covid df so it can be used as a cutoff
vac_start_date <- vac_start_df %>% dplyr::select(country, date.first.vac)
covid <- full_join(covid, vac_start_date, by ="country")

# cases & deaths up until vaccines start - USE THIS!
covid_prevac <- covid %>%
  group_by(country) %>%
  drop_na(date.first.vac) %>%
  subset(date < date.first.vac) %>%
  arrange(country,date) %>%
  slice_tail(n=1) %>%
  ungroup() %>%
  dplyr::select("country"| contains("case") | contains("death"))
colnames(covid_prevac)[2:19] <- paste(colnames(covid_prevac)[2:19], "prevac", sep = "_") #add suffix

#peak rates up until vaccinations start - USE THIS
covid_maxrate <- covid %>%
  drop_na(date.first.vac) %>%
  subset(date < date.first.vac) %>% 
  dplyr::select("country" | contains(c("new", "daily"))) %>%
  dplyr::select(-contains("vac")) %>% #drop vac cols
  group_by(country) %>%
  mutate_if(is.numeric, ~replace(., is.na(.), -1)) %>% #temporarily replace na's with -1
  summarise_if(is.numeric, max) %>%
  na_if(-1) %>% #set -1's back to na
  rename(daily.new.cases.per.100 = daily_new_cases_per_hundred,
         daily.new.deaths.per.100 = daily_new_deaths_per_hundred,
         monthly.new.cases.per.100 = monthly_new_cases_per_hundred,
         monthly.new.deaths.per.100 = monthly_new_deaths_per_hundred,
         weekly.new.cases.per.100 = weekly_new_cases_per_hundred,
         weekly.new.deaths.per.100 = weekly_new_deaths_per_hundred) #using . and _ separators to match WSD in case we want to pivot long later
colnames(covid_maxrate)[2:13] <- paste(colnames(covid_maxrate)[2:13], "max", sep = "_") #add suffix
```

Finally we'll merge these COVID predictors into a single dataframe.

```{r}
#df for cases and deaths
case.df_list <- list(covid_prevac, covid_maxrate)
covid.predict <- case.df_list %>% reduce(full_join, by='country') %>% as.data.frame() #join
#skim(covid.predict)
```


### Computing Vaccination Success Index

Since defining a country's success in administering vaccines is multifaceted, we'll construct the VSI on several summary metrics in addition to final vaccination counts. For instance, it is likely meaningful to account for how long it took each country to reach certain milestones (such as first starting to administer vaccines, administering X doses to each citizen, or vaccinating X% of the population) as well its peak rate of vaccination (the maximum rate of daily or weekly vaccinations) in determining whether a country did a "good job" in administering COVID vaccines to its populace. 

**much of this is from covid_EDA.Rmd and can be removed/cleaned up once Rmds are fully merged!**

```{r, warning=FALSE}
#days until country admins 1 dose per person
vac_1_dose <- covid %>%
  dplyr::select(country, date, vaccine_dose_events, date.first.vac) %>% #
  filter(vaccine_dose_events == "1_dose")%>%
  mutate(days.to.1d_rel = as.numeric(difftime(date, min(date), units = "days")),
         days.to.1d_abs = as.numeric(difftime(date, "2020-12-01", units = "days")), # setting Dec 1, 2020 as vaccine baseline to compare across vaccine events
         days.to.1d_fromstart = as.numeric(difftime(date, date.first.vac, units = "days"))
         ) 
  #vac_1_dose[which.min(vac_1_dose$date),] #first country to get one dose per person

#days until country admins 2 doses per person
vac_2_doses <- covid %>%
  dplyr::select(country, date, vaccine_dose_events, date.first.vac) %>%
  filter(vaccine_dose_events == "2_doses")%>%
  mutate(days.to.2d_rel = as.numeric(difftime(date, min(date), units = "days")),
         days.to.2d_abs = as.numeric(difftime(date, "2020-12-01", units = "days")),
         days.to.2d_fromstart = as.numeric(difftime(date, date.first.vac, units = "days"))
         )
  #vac_2_doses[which.min(vac_2_doses$date),] #first country to hit 2 doses

#days until country reports vaccinating 10% of pop
vac_10_df <- covid %>%
  dplyr::select(country, date, vaccination_events, date.first.vac) %>%
  filter(vaccination_events == "vac_10pct") %>%
  mutate(days.to.10pct_rel = as.numeric(difftime(date, min(date), units = "days")),
         days.to.10pct_abs = as.numeric(difftime(date, "2020-12-01", units = "days")),
         days.to.10pct_fromstart = as.numeric(difftime(date, date.first.vac, units = "days"))
         )
  #nrow(vac_10_df)

#days until country reports vaccinating 20% of pop
vac_20_df <- covid %>%
  dplyr::select(country, date, vaccination_events, date.first.vac) %>%
  filter(vaccination_events == "vac_20pct") %>%
  mutate(days.to.20pct_rel = as.numeric(difftime(date, min(date), units = "days")),
         days.to.20pct_abs = as.numeric(difftime(date, "2020-12-01", units = "days")),
         days.to.20pct_fromstart = as.numeric(difftime(date, date.first.vac, units = "days"))
         )
  #nrow(vac_20_df)

#days until country reports vaccinating 30% of pop
vac_30_df <- covid %>%
  dplyr::select(country, date, vaccination_events, date.first.vac) %>%
  filter(vaccination_events == "vac_30pct") %>%
  mutate(days.to.30pct_rel = as.numeric(difftime(date, min(date), units = "days")),
         days.to.30pct_abs = as.numeric(difftime(date, "2020-12-01", units = "days")),
         days.to.30pct_fromstart = as.numeric(difftime(date, date.first.vac, units = "days"))
         )
  #nrow(vac_30_df)

#days until country reports vaccinating 50% of pop
vac_50_df <- covid %>%
  dplyr::select(country, date, vaccination_events, date.first.vac) %>%
  filter(vaccination_events == "vac_50pct") %>%
  mutate(days.to.50pct_rel = as.numeric(difftime(date, min(date), units = "days")),
         days.to.50pct_abs = as.numeric(difftime(date, "2020-12-01", units = "days")),
         days.to.50pct_fromstart = as.numeric(difftime(date, date.first.vac, units = "days"))
         )

# mean daily rate of vaccinations per million ppl in first 6 months of each country's administration
vac_rate <- covid %>%
  dplyr::select(country, date, daily_vaccinations_per_million, date.first.vac) %>% 
  filter(daily_vaccinations_per_million > 0,
         date <= (date.first.vac %m+% months(6))) %>% # daily vaccinations in 1st 6 mo - make relative to their own vaccine start date
  drop_na(daily_vaccinations_per_million) %>% 
  group_by(country) %>%
  summarise(daily.vac.per.mil_6moavg = mean(daily_vaccinations_per_million))

# max rate of vaccinations administration
vac_maxrate <- covid %>%
  dplyr::select(country, date, daily_vaccinations_per_million, daily_vaccinations_raw, daily_vaccinations) %>% 
  group_by(country) %>%
  mutate_if(is.numeric, ~replace(., is.na(.), -1)) %>% #temporarily replace na's with -1
  summarise_if(is.numeric, max) %>%
  na_if(-1) %>% #set -1's back to na
  rename(daily.vac.raw_max = daily_vaccinations_raw,
         daily.vac_max = daily_vaccinations,
         daily.vac.per.mil_max = daily_vaccinations_per_million)
```

Now let's merge all these vaccination dfs into one dataframe so we can create a VSI to predict!

First we'll dump all the vaccine-related stuff into one big dataframe here:
``` {r}
#big df for vaccine info
last.vac <- last.timepoint %>% dplyr::select("country"| contains("vacc"))#get just vaccine stuff from  last.timepoint
vac.df_list <- list(vac_1_dose, vac_2_doses, vac_start_df, vac_10_df, vac_20_df, vac_30_df, vac_50_df, vac_rate, vac_maxrate, last.vac)
#drop date and event info since this is contained in col names
for (i in 1:length(vac.df_list)) {
  vac.df_list[[i]] <- vac.df_list[[i]] %>% dplyr::select(-c(contains("date") | contains("events")))
}

vac.dump <- vac.df_list %>% reduce(full_join, by='country') %>% as.data.frame() #join
#skim(vac.dump)
```

Now we'll pair it down to keep only key variables that will be meaningful in denoting a country's vaccination success. Variables were selected to be meaningful and non-redundant, and to have high completion rates. 

```{r}
vac.sum <- vac.dump %>% dplyr::select(
  country, 
  days.to.start_rel, #days until a country reports starting to administer vaccines, with the 1st country to start vaccinating as a baseline
  days.to.10pct_fromstart, #days between starting vaccinations and administering to 10% of pop
  daily.vac.per.mil_6moavg, #mean daily vaccinations in the first 6 months of administering
  people_vaccinated_per_hundred_last, #number of ppl vaccinated at last report (sometime in 2022)
  daily.vac.per.mil_max, #max number of vaccinations reported in a single day
  total_vaccinations_per_hundred_last,# number of vaccine doses per hundred ppl at last report (sometime in 2022)
  people_fully_vaccinated_per_hundred_last # number of people that received the full vaccine (typically 2 doses)
)

skim(vac.sum)
```

Next we'll perform PCA to create a score to summarize these important vaccination metrics into a continuous rating, the VSI. 

``` {r}
#drop NAs before PCA
vac.sum.pca <- na.omit(vac.sum)
nrow(vac.sum.pca)

vax.pca3 <- prcomp(x = vac.sum.pca[,-1], scale. = TRUE, center = TRUE)
vax.pca3$rotation
summary(vax.pca3) # adding 2 more vars brings the proportion of variance of PC1 from 67 to 73%, so keeping those in here
plot(summary(vax.pca3)$importance[2, ],  # PVE
     ylab="PVE",
     xlab="Number of PC's",
     pch = 16, 
     main="Scree Plot of PVE for Vaccination variables")
# Following the elbow rule, the first PC1 clearly explains a sufficient amount of variance. We will thus keep the first PC. 
vac.sum.with.VSI <- vac.sum.pca # copy the data
vac.sum.with.VSI$VSI <- -vax.pca3$x[,1] # append the VSI (inverse signed PC1 scores), where a positive nb means a country was good at getting people vaccinated
```

PC1 explains 73% of the variance and is therefore an excellent index indicating how well a country did at vaccinating their population; we'll take the inverse so that a higher VSI indicated a . We are calling it the "Vaccination Success Index", i.e. `VSI`. 

Let's see how the VSI loadings are distributed across countries: 

```{r}
# numerical summary
skim(vac.sum.with.VSI)
# plot sorted VSI scores
vac.sum.with.VSI %>%
  ggplot(aes(x = reorder(country, VSI), 
               y = VSI, fill = country)) +
    geom_bar(show.legend = FALSE, stat = "identity") +
    xlab("Country") +
    ylab("VSI") + 
    ggtitle("Vaccination Success Index score") +
    theme_bw() +
    theme(axis.text.x = element_text(angle = 60, hjust=1))
ggplot(vac.sum.with.VSI, aes(x = reorder(country, -VSI), y = VSI, fill = country)) + 
  geom_bar(stat = "identity")
```

We can see that Somalia appears to be the country with the lowest VSI, and Cuba appears to have the highest VSI. 


## Finalizing Dataset

We'll write our final dataset by joining our WSD predictors, prevax COVID case & death predictors, and VSI

```{r}
#merging WSD predictors with cases, deaths & maxrate data
case.wsd.wide <- inner_join(covid.predict, wsd.sum.wide, by="country") #predictors only
#skim(case.wsd.wide)

#adding in VSI
yval <- vac.sum.with.VSI %>% dplyr::select("country", "VSI")
final.df <- inner_join(case.wsd.wide, yval,  by=("country"))
#names(final.df)
n_unique(final.df$country)

##WRITE CSVs FOR MODELING##
#write.csv(case.wsd.wide, "data/case.wsd.wide-predictors.csv", row.names = FALSE)
#write.csv(final.df, "data/final.df.csv", row.names = FALSE)
```

## Modeling

Predicting inverse signed PC1 scores (from `data/VaccinationSucessIndexData-New.csv`) using `case.wsd.wide`.
Subdividing our dataset into `data.train`(60%), `data.test` (20%), and `data.val` (20%) DROP COUNTRY, CONTINENT & COUNTRY.CODE

```{r}
# Split the data 
N <- length(df$country)
n1 <- floor(.6*N)
n2 <- floor(.2*N)

set.seed(10)

# Split data to three portions of .6, .2 and .2 of data size N
idx_train <- sample(N, n1)
idx_no_train <- (which(! seq(1:N) %in% idx_train))
idx_test <- sample(idx_no_train, n2)
idx_val <- which(! idx_no_train %in% idx_test)
data.train <- as.data.frame(df[idx_train,])
data.test <- as.data.frame(df[idx_test,])
data.val <- as.data.frame(df[idx_val,])
data.train.test <- as.data.frame(df[-idx_val,]) #for methods that don't require separate test set
``` 

### Linear Models

Building a couple linear models:

Picking variables for backward selection:
* `Continent`
* `Electricity.Access_3yr` - since this would be related to infrastructure used for vaccination
* `cumulative_total_cases_per_hundred_prevac` - index of disease prevalence
* `cumulative_total_deaths_per_hundred_prevac` - index of disease severity
* `weekly.new.deaths.per.100_max` - max weekly death rate
* `GDP.PerCapita.Current_2018` - latest GDP per capita
* `UrbanPopulation.Prop_3yr` - how urban is pop
* `LifeExpenctancy_rel.diff` - change in health metric over time (tracks increase or decrease in pop health)
* `Trade.GDP_3yr` - see how much of economy is based in trade
* `CompulsoryEducationDurationYears_3yr` - level of education

Couple forward selections:
```{r}
fit1 <- lm(VSI ~ GDP.PerCapita.Current_2018, data.train)
Anova(fit1)
fit2 <- update(fit1, .~. + UrbanPopulation.Prop_3yr)
Anova(fit2)
fit3 <- update(fit2, .~. + cumulative_total_deaths_per_hundred_prevac)
Anova(fit3) #cumulative_total_deaths_per_hundred_prevac not significant


#try interactions
fit4 <- lm(VSI ~ GDP.PerCapita.Current_2018 + Continent + GDP.PerCapita.Current_2018*Continent, data.train)
Anova(fit4)

fit5 <- lm(VSI ~ GDP.PerCapita.Current_2018 + weekly.new.deaths.per.100_max + GDP.PerCapita.Current_2018*weekly.new.deaths.per.100_max, data.train)
Anova(fit5)
```


### Regsubsets

Running regsubsets:

```{r}
#subset data.train & data.test together since Cp will approx test error
data.fit1 <- data.train.test %>% dplyr::select(VSI, cumulative_total_cases_per_hundred_prevac, cumulative_total_deaths_per_hundred_prevac, weekly.new.deaths.per.100_max, Continent, Electricity.Access_3yr, GDP.PerCapita.Current_2018, UrbanPopulation.Prop_3yr, LifeExpenctancy_rel.diff, Trade.GDP_3yr, CompulsoryEducationDurationYears_3yr)

fit1 <- regsubsets(VSI ~., data.fit1 , nvmax=25, method="exhaustive")
f.e <- summary(fit1)
opt.size <- which.min(f.e$cp) #pick size with lowest cp
fit.exh.var <- f.e$which # logic indicators which variables are in
colnames(fit.exh.var)[fit.exh.var[opt.size,]][-1] # output variables selected
fit2 <- lm(VSI ~ Continent+Electricity.Access_3yr+GDP.PerCapita.Current_2018+CompulsoryEducationDurationYears_3yr, data.fit1)    # fit with selected variables
Anova(fit2) 
#note - not all significant but should stick with model minimizing Cp, right?

#check assumptions - NOTE - not great!
par(mfrow=c(1,2))
plot(fit2, 1) #linearity & homoscedasticity
plot(fit2, 2) #normality
```


### Neural Nets

Attempting neural nets:

**may need to set dummy vars for categorical**

Defining neural nets -   
  + normalization
  + two layers with 8 neurons in each layer
  + Activation function is `Relu`
  + Output layer is `linear`
  
Resources used:
https://www.tensorflow.org/tutorials/keras/regression
https://tensorflow.rstudio.com/reference/keras/layer_batch_normalization/
https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/
https://stackoverflow.com/questions/37232782/nan-loss-when-training-regression-network
https://rstudio-pubs-static.s3.amazonaws.com/344055_0c737f77c0ef4e36b6865cb843a7bb4a.html
https://keras.io/api/metrics/regression_metrics/

Imputing & setting x and y data as matrixes
```{r}
#skim(data.train.test)
#need to impute over NAs, using missForest for now
data.nn.train <- data.train.test %>% 
  mutate(across(where(is_character),as_factor)) %>% 
  dplyr::select(-c(Country.Code, country)) %>% #remove country
  select_if(funs(!is.Date(.)))
data.nn.trainimp <- missForest(data.nn.train, variablewise = TRUE)
data.nn.trainimp$ximp
data.nn.trainimp$OOBerror #high :(

#select only cols with missForest OOB MSE below 25%
good.predict <- which(data.nn.trainimp$OOBerror < 0.25) %>% unname()
nn.trainimp <- data.nn.trainimp$ximp[, c(good.predict)]
setdiff(names(data.nn.train), names(nn.trainimp)) #see what cols were lost
# dim(nn.trainimp) #imputing gives us 108 cols to work with
# dim(data.nn.train[ , colSums(is.na(data.nn.train)) == 0]) #as opposed to dropping all NA cols, which only gives us 67
# dim(na.omit(data.nn.train)) #or dropping rows with NAs, which would mean only 17 countries

## training input/y: need to be matrix/vector
data_xtrain <- nn.trainimp[, -99] %>% select_if(is.numeric)  #dim(data3_xtrain), remove y ~may need to set dummy vars - just grabbing numeric for now to test!
data_ytrain <- nn.trainimp[, 99]   # find y
data_xtrain <- as.matrix(data_xtrain) # make sure it it is a matrix
data_ytrain <- as.matrix(data_ytrain) # make sure it it is a matrix

## validation input/y
data_xval <- data.val[, -204]   #dim(data3_xtrain), remove y
data_yval <- data.val[, 204]   # find y
data_xval <- as.matrix(data_xval) # make sure it it is a matrix
data_yval <- as.matrix(data_yval) # make sure it it is a matrix
```
cumulative_total_deaths_last

Build and compile model
```{r}
# set seed for keras
set_random_seed(10)

#build model
p <- dim(data_xtrain)[2] # number of input variables (columns)
model <- keras_model_sequential() %>%
  layer_batch_normalization(input_shape = c(p)) %>% #recommended, not certain it's correct
  layer_dense(units = 12, activation = "relu") %>%
  # layer_dropout(0.25) %>% # drop 25% - https://stackoverflow.com/questions/37232782/nan-loss-when-training-regression-network
  layer_dense(units = 12, activation = "relu") %>%  
  # layer_dropout(0.25) %>% # drop 25%
  layer_dense(units = 1, activation = "linear") # output = 1 unit for linear(?) regression
print(model)

# #compile the Model (based on https://www.tensorflow.org/tutorials/keras/regression)
# model %>% compile(
#   optimizer = "adam",
#   loss = "mean_squared_error",
#   metrics = list("mean_squared_error"),
# )

#compile model (based on https://tensorflow.rstudio.com/tutorials/beginners/basic-ml/tutorial_basic_regression/)
model %>%
  compile(
    loss = "mse",
    optimizer = optimizer_rmsprop(),
    metrics = c("accuracy")
  )
```

Fitting model

```{r}
set_random_seed(10)
fit.nn <- model %>% fit(
  data_xtrain,
  data_ytrain,
  epochs = 100,
  batch_size = 20,
  validation_split = .15 # set 15% of the data_xtrain, data_ytrain as the validation data
)

plot(fit.nn)
```

# Appendix

## Unused Dataframes
Stuff that we output but wasn't used for modeling - may be useful for EDA or post-hoc analyses:

```{r, eval=FALSE}
#get just cases & deaths stuff from  last.timepoint (mar 2022 covid cases and death)
last.case <- last.timepoint %>% dplyr::select("country", "date_last" | contains("case") | contains("death")) 

# cases & deaths at 1 yr after date.first.vac
covid_1yr <- covid %>%
  group_by(country) %>%
  drop_na(date.first.vac) %>%
  subset(date == (date.first.vac %m+% months(12))) %>%
  dplyr::select("country"| contains("case") | contains("death"))
colnames(covid_1yr)[2:19] <- paste(colnames(covid_1yr)[2:19], "1yr", sep = "_") #add suffix
#covid_1yr <- covid_1yr[, which(colMeans(!is.na(covid_1yr)) > 0.5)] #remove cols with NA > 50%
#sum(is.na(covid_1yr$cumulative_total_cases))

#merging all COVID data 
covid.df_list <- list(covid_prevac, covid_maxrate, last.case, vac.dump, covid_1yr)
covid.all <- covid.df_list %>% reduce(full_join, by='country') %>% as.data.frame() #join
n_unique(covid.all$country)
skim(covid.all)

#merging WSD predictors with vaccination summary data
vac.wsd.wide <- inner_join(vac.dump, wsd.sum.wide, by="country")
#skim(vac.wsd.wide)

#merging WSD predictors with latest COVID data (~March 17, 2022)
mar22.wsd.wide <- inner_join(last.timepoint, wsd.sum.wide, by="country")
#skim(mar22.wsd.wide)

#merging everything!
wsd.covid.all <- inner_join(covid.all, wsd.sum.wide, by="country")
skim(wsd.covid.all)
```

