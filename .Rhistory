group_by(country) %>%
filter(date == max(date)) %>%   # keep last reported date for every country
filter(year == 2022) %>% # keep only countries who last reported in 2022 (as we are interested in a recent vaccination status)
# filter(month_year == "2022-03-01") # could also filter for march 2022 specifically
dplyr::select(country, date, missing_vac_cols) %>% # keep only variables not yet included in last.timepoint
rename(date.vac = date)# rename the date to correspond to last reported vaccine info
# merge the last reported vaccination info with last reported cases/deaths info
last.timepoint <- merge(last.timepoint_filt, vac_info_2022, by = "country")
colnames(last.timepoint)[2:43] <- paste(colnames(last.timepoint)[2:43], "last", sep = "_") #add suffix
#names(last.timepoint)
# #find date of vaccination events
# table(covid$vaccination_events)
# table(covid$vaccine_dose_events)
summary(last.timepoint$date.vac_last)
# find the date each country reported starting to vaccinate
vac_start_df <- covid %>%
dplyr::select(country, date, vaccination_events) %>%
filter(vaccination_events == "vac_start") %>%
mutate(days.to.start_rel = as.numeric(difftime(date, min(date), units = "days")),
days.to.start_abs = as.numeric(difftime(date, "2020-12-01", units = "days"))) %>%
rename(date.first.vac = date)
# adding date of first vaccination for each country back to the covid df so it can be used as a cutoff
vac_start_date <- vac_start_df %>% dplyr::select(country, date.first.vac)
covid <- full_join(covid, vac_start_date, by ="country")
# cases & deaths up until vaccines start - USE THIS!
covid_prevac <- covid %>%
group_by(country) %>%
drop_na(date.first.vac) %>%
subset(date < date.first.vac) %>%
arrange(country,date) %>%
slice_tail(n=1) %>%
ungroup() %>%
dplyr::select("country"| contains("case") | contains("death"))
colnames(covid_prevac)[2:19] <- paste(colnames(covid_prevac)[2:19], "prevac", sep = "_") #add suffix
#peak rates up until vaccinations start - USE THIS
covid_maxrate <- covid %>%
drop_na(date.first.vac) %>%
subset(date < date.first.vac) %>%
dplyr::select("country" | contains(c("new", "daily"))) %>%
dplyr::select(-contains("vac")) %>% #drop vac cols
group_by(country) %>%
mutate_if(is.numeric, ~replace(., is.na(.), -1)) %>% #temporarily replace na's with -1
summarise_if(is.numeric, max) %>%
na_if(-1) %>% #set -1's back to na
rename(daily.new.cases.per.100 = daily_new_cases_per_hundred,
daily.new.deaths.per.100 = daily_new_deaths_per_hundred,
monthly.new.cases.per.100 = monthly_new_cases_per_hundred,
monthly.new.deaths.per.100 = monthly_new_deaths_per_hundred,
weekly.new.cases.per.100 = weekly_new_cases_per_hundred,
weekly.new.deaths.per.100 = weekly_new_deaths_per_hundred) #using . and _ separators to match WSD in case we want to pivot long later
colnames(covid_maxrate)[2:13] <- paste(colnames(covid_maxrate)[2:13], "max", sep = "_") #add suffix
#df for cases and deaths
case.df_list <- list(covid_maxrate, covid_prevac) #ordered to prioritize when removing correlated vars
covid.predict <- case.df_list %>% reduce(full_join, by='country') %>% as.data.frame() #join
#skim(covid.predict)
#removing 1 of each very colinear pair, method from https://statisticsglobe.com/remove-highly-correlated-variables-from-data-frame-r
#stash year and non-numeric variables (just country)
covid.fac <- covid.predict %>%
select(negate(is.numeric))
#plot
# create correlation plot to see whether some variables are highly correlated (not tolerated by lm)
covid.predict %>%
select_if(., is.numeric) %>%
cor(., use = "pairwise.complete.obs") %>%
corrplot(method = "color",
type = "upper", order = "hclust",
addCoef.col = "black", # Add coefficient of correlation
tl.col="black", tl.srt = 45, #Text label color and rotation
# hide correlation coefficient on the principal diagonal
diag = FALSE
)
#get colinearities
covid.num <- covid.predict %>%
select(-contains("Year")) %>%
select_if(., is.numeric)
covid.corr <- covid.num %>%
cor(., use="pairwise.complete.obs")
#removing upper triangle
covid.corr[upper.tri(covid.corr)] <- 0
diag(covid.corr) <- 0
#remove vars at abs(0.97) threshold - keeping high to preserve as many variables as possible (also b/c we expect things to be correlated IRL)
covid.list <-  apply(covid.corr, 2, function(x) any(abs(x) > 0.97)) #list highly correlated variables
covid.nocor <- covid.num[ , !covid.list]
#add back factors
covid.nocor <- cbind(covid.fac, covid.nocor)
dim(covid.nocor)
head(covid.nocor)
#days until country admins 1 dose per person
vac_1_dose <- covid %>%
dplyr::select(country, date, vaccine_dose_events, date.first.vac) %>% #
filter(vaccine_dose_events == "1_dose")%>%
mutate(days.to.1d_rel = as.numeric(difftime(date, min(date), units = "days")),
days.to.1d_abs = as.numeric(difftime(date, "2020-12-01", units = "days")), # setting Dec 1, 2020 as vaccine baseline to compare across vaccine events
days.to.1d_fromstart = as.numeric(difftime(date, date.first.vac, units = "days"))
)
#vac_1_dose[which.min(vac_1_dose$date),] #first country to get one dose per person
#days until country admins 2 doses per person
vac_2_doses <- covid %>%
dplyr::select(country, date, vaccine_dose_events, date.first.vac) %>%
filter(vaccine_dose_events == "2_doses")%>%
mutate(days.to.2d_rel = as.numeric(difftime(date, min(date), units = "days")),
days.to.2d_abs = as.numeric(difftime(date, "2020-12-01", units = "days")),
days.to.2d_fromstart = as.numeric(difftime(date, date.first.vac, units = "days"))
)
#vac_2_doses[which.min(vac_2_doses$date),] #first country to hit 2 doses
#days until country reports vaccinating 10% of pop
vac_10_df <- covid %>%
dplyr::select(country, date, vaccination_events, date.first.vac) %>%
filter(vaccination_events == "vac_10pct") %>%
mutate(days.to.10pct_rel = as.numeric(difftime(date, min(date), units = "days")),
days.to.10pct_abs = as.numeric(difftime(date, "2020-12-01", units = "days")),
days.to.10pct_fromstart = as.numeric(difftime(date, date.first.vac, units = "days"))
)
#nrow(vac_10_df)
#days until country reports vaccinating 20% of pop
vac_20_df <- covid %>%
dplyr::select(country, date, vaccination_events, date.first.vac) %>%
filter(vaccination_events == "vac_20pct") %>%
mutate(days.to.20pct_rel = as.numeric(difftime(date, min(date), units = "days")),
days.to.20pct_abs = as.numeric(difftime(date, "2020-12-01", units = "days")),
days.to.20pct_fromstart = as.numeric(difftime(date, date.first.vac, units = "days"))
)
#nrow(vac_20_df)
#days until country reports vaccinating 30% of pop
vac_30_df <- covid %>%
dplyr::select(country, date, vaccination_events, date.first.vac) %>%
filter(vaccination_events == "vac_30pct") %>%
mutate(days.to.30pct_rel = as.numeric(difftime(date, min(date), units = "days")),
days.to.30pct_abs = as.numeric(difftime(date, "2020-12-01", units = "days")),
days.to.30pct_fromstart = as.numeric(difftime(date, date.first.vac, units = "days"))
)
#nrow(vac_30_df)
#days until country reports vaccinating 50% of pop
vac_50_df <- covid %>%
dplyr::select(country, date, vaccination_events, date.first.vac) %>%
filter(vaccination_events == "vac_50pct") %>%
mutate(days.to.50pct_rel = as.numeric(difftime(date, min(date), units = "days")),
days.to.50pct_abs = as.numeric(difftime(date, "2020-12-01", units = "days")),
days.to.50pct_fromstart = as.numeric(difftime(date, date.first.vac, units = "days"))
)
# mean daily rate of vaccinations per million ppl in first 6 months of each country's administration
vac_rate <- covid %>%
dplyr::select(country, date, daily_vaccinations_per_million, date.first.vac) %>%
filter(daily_vaccinations_per_million > 0,
date <= (date.first.vac %m+% months(6))) %>% # daily vaccinations in 1st 6 mo - make relative to their own vaccine start date
drop_na(daily_vaccinations_per_million) %>%
group_by(country) %>%
summarise(daily.vac.per.mil_6moavg = mean(daily_vaccinations_per_million))
# max rate of vaccinations administration
vac_maxrate <- covid %>%
dplyr::select(country, date, daily_vaccinations_per_million, daily_vaccinations_raw, daily_vaccinations) %>%
group_by(country) %>%
mutate_if(is.numeric, ~replace(., is.na(.), -1)) %>% #temporarily replace na's with -1
summarise_if(is.numeric, max) %>%
na_if(-1) %>% #set -1's back to na
rename(daily.vac.raw_max = daily_vaccinations_raw,
daily.vac_max = daily_vaccinations,
daily.vac.per.mil_max = daily_vaccinations_per_million)
#big df for vaccine info
last.vac <- last.timepoint %>% dplyr::select("country"| contains("vacc"))#get just vaccine stuff from  last.timepoint
vac.df_list <- list(vac_1_dose, vac_2_doses, vac_start_df, vac_10_df, vac_20_df, vac_30_df, vac_50_df, vac_rate, vac_maxrate, last.vac)
#drop date and event info since this is contained in col names
for (i in 1:length(vac.df_list)) {
vac.df_list[[i]] <- vac.df_list[[i]] %>% dplyr::select(-c(contains("date") | contains("events")))
}
vac.dump <- vac.df_list %>% reduce(full_join, by='country') %>% as.data.frame() #join
#skim(vac.dump)
vac.sum <- vac.dump %>% dplyr::select(
country,
days.to.start_rel, #days until a country reports starting to administer vaccines, with the 1st country to start vaccinating as a baseline
days.to.10pct_fromstart, #days between starting vaccinations and administering to 10% of pop
daily.vac.per.mil_6moavg, #mean daily vaccinations in the first 6 months of administering
people_vaccinated_per_hundred_last, #number of ppl vaccinated (at least once) at last report (sometime in 2022)
daily.vac.per.mil_max, #max number of vaccinations reported in a single day
total_vaccinations_per_hundred_last,# number of vaccine doses per hundred ppl at last report (sometime in 2022)
people_fully_vaccinated_per_hundred_last # number of people that received the full vaccine (typically 2 doses)
)
skim(vac.sum)
#drop NAs before PCA
vac.sum.pca <- vac.sum %>% na.omit()
nrow(vac.sum.pca)
vax.pca3 <- prcomp(x = vac.sum.pca[,-1], scale. = TRUE, center = TRUE)
vax.pca3$rotation
summary(vax.pca3) # PVE of PC1=73.3%
plot(summary(vax.pca3)$importance[2, ],  # PVE
ylab="PVE",
xlab="Number of PC's",
pch = 16,
main="Scree Plot of PVE for Vaccination variables")
# Following the elbow rule, the first PC1 clearly explains a sufficient amount of variance. We will thus keep the first PC.
vac.sum.with.VSI <- vac.sum.pca # copy the data
vac.sum.with.VSI$VSI <- -vax.pca3$x[,1] # append the VSI (inverse signed PC1 scores), where a positive nb means a country was good at getting people vaccinated
# numerical summary
skim(vac.sum.with.VSI)
# plot sorted VSI scores
vac.sum.with.VSI %>%
ggplot(aes(x = reorder(country, VSI),
y = VSI, fill = country)) +
geom_bar(show.legend = FALSE, stat = "identity") +
xlab("Country") +
ylab("VSI") +
ggtitle("Vaccination Success Index score") +
theme_bw() +
theme(axis.text.x = element_text(angle = 60, hjust=1))
ggplot(vac.sum.with.VSI, aes(x = reorder(country, -VSI), y = VSI, fill = country)) +
geom_bar(stat = "identity")
#summary(vac.sum.with.VSI$VSI)#VSI range
#merging WSD predictors with cases, deaths & maxrate data
case.wsd.wide <- inner_join(wsd.nocor, covid.nocor, by="country") #predictors only
#skim(case.wsd.wide)
#adding in VSI
yval <- vac.sum.with.VSI %>% dplyr::select("country", "VSI")
final.df <- inner_join(case.wsd.wide, yval,  by=("country"))
#names(final.df)
n_unique(final.df$country)
##WRITE CSVs FOR MODELING##
#write.csv(case.wsd.wide, "data/case.wsd.wide-predictors.csv", row.names = FALSE)
#write.csv(final.df, "data/final.df.csv", row.names = FALSE)
# Split the data
N <- length(final.df$country)
n1 <- floor(.85*N)
n2 <- floor(.15*N)
#dropping vars we don't want to predict on
df.pred <- final.df %>% dplyr::select(-c(country, Continent, Country.Code, Year_latest, nYears_avg)) #drop years!
set.seed(10)
# Split data
idx_train <- sample(N, n1)
idx_val <- (which(! seq(1:N) %in% idx_train))
data.train <- as.data.frame(df.pred[idx_train,])
data.val <- as.data.frame(df.pred[idx_val,])
# economic factors lm
lm.econ.1 <- lm(data = data.train, formula = VSI ~ ExportGoodsServices.GDP_avg +
FinalConsumptionExpenditure.GDP_avg +
GDP.Current_avg +
GDP.PerCapita.Current_avg +
ConsumerPriceInflation_avg)
summary(lm.econ.1)
#Anova(lm.econ.1)
#commented b/c fit using the 60% data.train, will need to be checked:
# # remove non-significant variables w backward selection
# lm.econ.1.refined <- update(lm.econ.1, .~. -FinalConsumptionExpenditure.GDP_avg)
# #Anova(lm.econ.1.refined)
# lm.econ.2.refined <- update(lm.econ.1.refined, .~. -ExportGoodsServices.GDP_avg)
# #Anova(lm.econ.2.refined)
# lm.econ.3.refined <- update(lm.econ.2.refined, .~. -GDP.Current_avg )
# #Anova(lm.econ.3.refined)
# lm.econ.4.refined <- update(lm.econ.3.refined, .~. -ConsumerPriceInflation_avg)
# Anova(lm.econ.4.refined)
# build covid model
lm.covid.1 <- lm(VSI ~ cumulative_total_cases_per_hundred_prevac +
cumulative_total_deaths_per_hundred_prevac +
monthly.new.cases.per.100_max +
monthly.new.deaths.per.100_max +
monthly_new_cases_per_hundred_prevac, data = data.train)
summary(lm.covid.1)
Anova(lm.covid.1)
#LOL only the intercept is significant, going to do some forward selection - start with cumulative_total_cases_per_hundred_prevac!
lm.covid.death <- lm(VSI ~ cumulative_total_deaths_per_hundred_prevac, data = data.train)
summary(lm.covid.death)
lm.covid.f2 <- update(lm.covid.death, .~. + cumulative_total_cases_per_hundred_prevac)
summary(lm.covid.f2) #tried a few different options, adding more COVID vars don't seem to be predictive
Anova(lm.covid.f2)
# based on Anova, looks like cumulative_total_cases_per_hundred_prevac is the way to go - forward selection on this!
lm.covid.case <- lm(VSI ~ cumulative_total_cases_per_hundred_prevac, data = data.train)
summary(lm.covid.case)
# build political & country development index model
lm.political.1 <- lm(VSI ~ WorldRegion +
Electricity.Access_avg +
GDP.PerCapita.Current_avg +
UrbanPopulation.Prop_avg +
LifeExpenctancy_avg +
CompulsoryEducationDurationYears_latest +
WomenInBusinessLawIndex_avg +
WorldBankIncomeClass_avg +
RegimeType_avg +
IndividualsUsingInternet_latest,
data = data.train)
summary(lm.political.1)
Anova(lm.political.1)
#commented b/c fit using the 60% data.train, will need to be checked:
# # remove non-significant variables w backward selection
# lm.political.1.refined <- update(lm.political.1, .~. -GDP.PerCapita.Current_avg)
# #Anova(lm.political.1.refined)
# lm.political.2.refined <- update(lm.political.1.refined, .~. -WorldBankIncomeClass_avg)
# #Anova(lm.political.2.refined)
# lm.political.3.refined <- update(lm.political.2.refined, .~. -UrbanPopulation.Prop_avg)
# #Anova(lm.political.3.refined)
# lm.political.4.refined <- update(lm.political.3.refined, .~. -Electricity.Access_avg)
# #Anova(lm.political.4.refined)
# lm.political.5.refined <- update(lm.political.4.refined, .~. -IndividualsUsingInternet_latest)
# #Anova(lm.political.5.refined)
# lm.political.6.refined <- update(lm.political.5.refined, .~. -CompulsoryEducationDurationYears_latest)
# Anova(lm.political.6.refined)
# fit using the 60% data.train, will need to be checked:
# build full model
lm.full <- lm(VSI ~ GDP.PerCapita.Current_avg +
cumulative_total_cases_per_hundred_prevac +
WorldRegion +
LifeExpenctancy_avg +
WomenInBusinessLawIndex_avg +
RegimeType_avg,
data = data.train)
Anova(lm.full)
summary(lm.full)
# remove non-significant variables
lm.full.1.refined <- update(lm.full, .~. -cumulative_total_cases_per_hundred_prevac)
Anova(lm.full.1.refined)
lm.full.2.refined <- update(lm.full.1.refined, .~. -GDP.PerCapita.Current_avg)
Anova(lm.full.2.refined)
summary(lm.full.2.refined)
# plotting model diagnosis plots
plot(lm.full.2.refined, 1:2)
# get life expectancy p value
lm.full.2.refined.summ <- summary(lm.full.2.refined)
pval <- lm.full.2.refined.summ$coefficients[["LifeExpenctancy_avg", "Pr(>|t|)"]]
# get life expectancy correlation with VSI
cor_value <- cor(data.train$LifeExpenctancy_avg, data.train$VSI, use = "complete.obs")
plot_VSI_lifexp <- final.df %>% # using full df here to include country information
ggplot(aes(x = LifeExpenctancy_avg, y = VSI)) +
geom_point(aes(group = country, color = country), show.legend = FALSE) +
geom_smooth(method = "lm", color = "grey") +
theme_bw() +
ggtitle("Association between life expectancy and VSI") +
xlab("Life expectancy (average)") +
annotate("text", x = 56, y = max(final.df["VSI"]) - 0.5, # position
size = 4, # font size
label = paste0("Correlation: ", sprintf("%.3f", cor_value), "\n", # correlation label
"p-value = ", sprintf("%.3f", pval))) # pvalue label
plot_VSI_lifexp
# ggplotly(plot_VSI_lifexp) # generate interactive plot
# remove all rows containing NAs (LASSO does not work without this step) - lost 9 rows :(
data.train.sub <- na.omit(data.train)
nrow(data.train.sub)
# create matrices to feed into gmnet
Y <- as.matrix(data.train.sub[, 61]) # extract Y (VSI)
X <- model.matrix(VSI~., data = data.train.sub)[, -61] # remove the first (interecept) column with only 1s
# to control the randomness in K folds
set.seed(10)
# run LASSO
lasso.avg <- cv.glmnet(X, Y, alpha = 1, nfolds = 10, intercept = TRUE)
# plot LASSO results
plot(lasso.avg)
lasso.avg$lambda.1se # use lambda.1se to select the smaller model
coef.1se <- coef(lasso.avg, s = "lambda.1se") # get coefficients
coef.1se <- coef.1se[which(coef.1se != 0),] # show variables with non-zero coefficients
# output variable names
coef.1se <- rownames(as.matrix(coef.1se))[-1]
coef.1se
# output lm based on LASSO
coef.1se <- coef(lasso.avg, s="lambda.1se")  #s=c("lambda.1se","lambda.min") or lambda value
coef.1se <- coef.1se[which(coef.1se != 0),]   # get the non=zero coefficients
var.1se <- rownames(as.matrix(coef.1se))[-1] # output the variable names without intercept
wsd.avg.vsi.lasso <-  data.train.sub %>%
select(c("VSI", var.1se)) # get a subset with response and LASSO output
# run relaxed LASSO
fit.1se.lm <- lm(VSI~., data = wsd.avg.vsi.lasso)
summary(fit.1se.lm)
single.tree1 <- tree(VSI ~ ., data.train.sub, control=tree.control(nobs=nrow(data.train.sub),
minsize=4,
mindev=0.009))
plot(single.tree1)
text(single.tree1, pretty = 1)
title("Single Tree | VSI Prediction")
set.seed(1) # for reproducibility, we set the seed
fit.1.rf <- randomForest(VSI~., data.train.sub, mtry=6, ntree=500)
plot(fit.1.rf$mse, xlab="Number of Trees", col="red", ylab="OOB MSE")
title(main = "OOB Test Error by Number of Trees Used")
set.seed(10)
max.mtry <- ncol(data.train.sub)
error.p <- 1:max.mtry
for (p in 1:max.mtry)
{
rf.temp <- randomForest(VSI~., data.train.sub, mtry=p, ntree=100)
error.p[p] <- rf.temp$mse[100]
}
plot(1:max.mtry, error.p,
main = "Testing Errors of mtry with 100 trees",
xlab="mtry",
ylab="OOB MSE")
lines(1:max.mtry, error.p)
set.seed(2)
fit.final.rf <- randomForest(VSI~., data.train.sub, mtry=30, ntree=100)
plot(fit.final.rf)
regsub.fit1 <- regsubsets(VSI ~., data.train , nvmax=25, method="forward", really.big = TRUE)
f.e <- summary(regsub.fit1)
plot(f.e$cp, xlab="Number of predictors",
ylab="Cp", col="red", pch=16)
opt.size <- which.min(f.e$cp) #pick size with lowest cp
regsub.fit.var <- f.e$which # logic indicators which variables are in
regsub.vars <- colnames(regsub.fit.var)[regsub.fit.var[opt.size,]][-1] # output variables selected
regsub.vars
fit1.lm <- lm(VSI ~ WorldRegion +
WorldBankIncomeClass_latest +
WorldBankIncomeClass_avg +
Govt.FinalConsumptionExpenditure.GDP_latest +
WomenInBusinessLawIndex_avg +
LifeExpenctancy_avg +
cumulative_total_cases_prevac +
weekly_new_deaths_per_hundred_prevac, data.train)
Anova(fit1.lm)
fit1.lm.refined <- update(fit1.lm, .~. -weekly_new_deaths_per_hundred_prevac)
Anova(fit1.lm.refined)
fit1.lm.2.refined <- update(fit1.lm.refined, .~. -WorldBankIncomeClass_avg)
Anova(fit1.lm.2.refined)
fit1.lm.3.refined <- update(fit1.lm.2.refined, .~. -cumulative_total_cases_prevac)
Anova(fit1.lm.3.refined)
#skim(data.train.test)
#need to impute over NAs, using missForest for now
df.pred.nn <- df.pred %>%
mutate(across(where(is_character),as_factor))
df.pred.imp <- missForest(df.pred.nn) #variablewise = TRUE
df.pred.imp$OOBerror #looks good!
nn.imp <- as.data.frame(df.pred.imp$ximp)
dim(nn.imp) #imputing retains all 61 cols
dim(df.pred[ , colSums(is.na(df.pred)) == 0]) #as opposed to dropping all NA cols, which only gives us 41
dim(na.omit(df.pred)) #or dropping rows with NAs, which would mean only 87 countries
#set dummy variables for categorical
nn.imp.dummy <- dummy_cols(nn.imp, remove_first_dummy = TRUE) %>% #setting remove_first_dummy = TRUE to have base levels
select_if(funs(!is.factor(.))) #drop original factors
#splitting into train and val using same indices from original data.train and data.val split
nn.train <- as.data.frame(nn.imp.dummy[idx_train,])
nn.val <- as.data.frame(nn.imp.dummy[idx_val,])
## training input/y: need to be matrix/vector
data_xtrain <- nn.train %>% dplyr::select(-VSI) %>% as.matrix()
data_ytrain <- nn.train %>% dplyr::select(VSI) %>% as.matrix()  # find y
# ## validation input/y
data_xval <- nn.val %>% dplyr::select(-VSI) %>% as.matrix()
data_yval <- nn.val %>% dplyr::select(VSI) %>% as.matrix()
# set seed for keras
set_random_seed(10)
#build model
p <- dim(data_xtrain)[2] # number of input variables (columns)
model <- keras_model_sequential() %>%
layer_batch_normalization(input_shape = c(p)) %>% #recommended, not certain it's correct
layer_dense(units = 32, activation = "relu") %>%
layer_dropout(0.25) %>% # https://stackoverflow.com/questions/37232782/nan-loss-when-training-regression-network
layer_dense(units = 12, activation = "relu") %>%
layer_dropout(0.25) %>%
layer_dense(units = 1, activation = "linear") # output = 1 unit for linear(?) regression
print(model)
#compile mode
model %>%
compile(
loss = "mse",
optimizer = optimizer_rmsprop(), #alternative "adam"
metrics = list("mean_absolute_error")
)
set_random_seed(10)
fit.nn <- model %>% fit(
data_xtrain,
data_ytrain,
epochs = 150,
batch_size = 8,
validation_split = .15 # set 15% of the data_xtrain, data_ytrain as the validation data
)
plot(fit.nn)
min.epoch <- which.min(fit.nn$metrics$val_loss)
fit.nn$metrics$val_loss[min.epoch]
fit.nn$metrics$loss[min.epoch]
# set seed for keras
set_random_seed(10)
#build model
p <- dim(data_xtrain)[2] # number of input variables (columns)
model2 <- keras_model_sequential() %>%
layer_batch_normalization(input_shape = c(p)) %>% #recommended, not certain it's correct
layer_dense(units = 32, activation = "relu") %>%
layer_dropout(0.25) %>%
layer_dense(units = 12, activation = "relu") %>%
layer_dropout(0.25) %>%
layer_dense(units = 1, activation = "linear") # output = 1 unit for linear(?) regression
print(model2)
#compile mode
model2 %>%
compile(
loss = "mse",
optimizer = optimizer_rmsprop(), #alternative "adam"
metrics = list("mean_absolute_error")
)
set_random_seed(10)
fit.nn.full <- model2 %>% fit(
data_xtrain,
data_ytrain,
epochs = min.epoch,
batch_size = 8)
fit.nn.full$metrics$loss[min.epoch]
# average probabilities
df.yhat <- data.train %>% dplyr::select(VSI) #initialize df to store outputs
df.yhat$full.lm <- predict(lm.full, data.train)
df.yhat$lasso <- predict(fit.1se.lm, data.train)
df.yhat$rf <- predict(fit.final.rf, data.train)
df.yhat$regsub <- predict(fit1.lm, data.train)
nn.list <- model2 %>% predict(data_xtrain) %>% unname()
df.yhat <- cbind(df.yhat, as.data.frame(nn.list))
df.yhat <- df.yhat %>%
mutate(en.pred = rowMeans(df.yhat[ , c(2:6)], na.rm=TRUE))
en.train.mse <- mean((df.yhat$VSI - df.yhat$en.pred)^2)
en.train.mse
set.seed(10)
max.mtry <- ncol(data.train.sub)
error.p <- 1:max.mtry
for (p in 1:max.mtry)
{
rf.temp <- randomForest(VSI~., data.train.sub, mtry=p, ntree=100)
error.p[p] <- rf.temp$mse[100]
}
plot(1:max.mtry, error.p,
main = "Testing Errors of mtry with 100 trees",
xlab="mtry",
ylab="OOB MSE")
lines(1:max.mtry, error.p)
sqrt(.461)
fit.final.rf$mse[100]
